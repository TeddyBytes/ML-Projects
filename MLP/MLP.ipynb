{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./names.txt\") as file:\n",
    "    words = file.readlines()\n",
    "# print(words)\n",
    "words = [word.strip() for word in words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store alphabet from data\n",
    "alphabet = set()\n",
    "\n",
    "# For every character of each word\n",
    "for word in words:\n",
    "    for char in word:\n",
    "        alphabet.add(char)\n",
    "        \n",
    "# Add start and end tokens\n",
    "alphabet.add('<S>')\n",
    "alphabet.add('<E>')\n",
    "alphabet = sorted(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store numeric conversions of tokens\n",
    "tok_to_int = dict()\n",
    "int_to_tok = dict()\n",
    "\n",
    "# Iterate across entire alphabet\n",
    "for i, token in enumerate(alphabet):\n",
    "    tok_to_int[token] = i\n",
    "    int_to_tok[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Window Size \n",
    "window_size = 3\n",
    "\n",
    "# Store training vardiables\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "#Iterate over each word\n",
    "for word in words:\n",
    "\n",
    "    # Store initial window of padding\n",
    "    window = [tok_to_int[\"<S>\"] for _ in range(window_size)]\n",
    "    # Add ending token to word\n",
    "    word = list(word) + ['<E>']\n",
    "\n",
    "    # Store edge case training point\n",
    "    X.append(window) \n",
    "    # Store edge case target\n",
    "    Y.append(tok_to_int[word[0]]) #T\n",
    "    # Iterate over each character + padding\n",
    "    for char1, char2 in zip(word[:], word[1:]):\n",
    "        # Store training data and targets\n",
    "        # print(window)\n",
    "        window = window[1:] + [tok_to_int[char1]]\n",
    "        # Store window in training set\n",
    "        X.append(window)\n",
    "        # print(tok_to_int[char2])\n",
    "        Y.append(tok_to_int[char2])\n",
    "\n",
    "        \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "len_of_emb = 2\n",
    "nuerons_in_layer = 3\n",
    "\n",
    "# Store words embeddings with 2 features\n",
    "C = torch.rand(len(alphabet), len_of_emb)\n",
    "\n",
    "# Store dataset via embeddings\n",
    "embed = C[X].view(-1, window_size*len_of_emb).to(torch.float64)\n",
    "\n",
    "\n",
    "# Store weights and bias\n",
    "W1 = torch.rand(nuerons_in_layer, nuerons_in_layer*len_of_emb, requires_grad=True, dtype=torch.float64)\n",
    "B1 = torch.rand(228146, requires_grad=True, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 6]), torch.Size([6, 228146]), torch.Size([228146]))"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape,embed.T.shape, B1.shape, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (W1 @ embed.T) + B1\n",
    "# result = 3 x 228146\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8598, 0.9068, 0.8778,  ..., 0.7654, 0.9712, 0.6195],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
