{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewaudley/Documents/Machine Learning Projects/Business Recommendation System/brs/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Data processing and numerical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and recommendation libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml import Pipeline\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "\n",
    "# IPython for displaying outputs\n",
    "from IPython.display import display\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split data once - no temporal features\n",
    "# train, temp = spark_df.randomSplit([0.75, 0.25], seed=42)\n",
    "# val, test = temp.randomSplit([.15, .10], seed=42)\n",
    "# # \n",
    "\n",
    "# # define file paths (relative to the current directory)\n",
    "# model_data_path = \"../../data/interim/\"\n",
    "\n",
    "# # save each DataFrame in parquet format\n",
    "# train.write.parquet(os.path.join(model_data_path, f\"train_set.parquet\"), mode='overwrite')\n",
    "# val.write.parquet(os.path.join(model_data_path, f\"val_set.parquet\"), mode='overwrite')\n",
    "# test.write.parquet(os.path.join(model_data_path, f\"test_set.parquet\"), mode='overwrite')\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# ========================================\n",
    "# Open sessions for necessary packages\n",
    "# ========================================\n",
    "# spark = None\n",
    "\n",
    "# def open_session(close=False):\n",
    "#     global spark  # \n",
    "#     if not close:\n",
    "#         if spark is None or spark.sparkContext is None:\n",
    "#             spark = SparkSession.builder \\\n",
    "#                 .appName(\"ALS in Spark\") \\\n",
    "#                 .getOrCreate()\n",
    "#             # set up MLflow (only needs to be done once)\n",
    "#     else:\n",
    "#         if spark is not None:\n",
    "#             spark.stop()\n",
    "#             spark = None\n",
    "            \n",
    "# # open_session()\n",
    "# open_session(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "# model_data_path = \"../data/interim/\"\n",
    "\n",
    "# train = spark.read.parquet(os.path.join(model_data_path, \"train_set.parquet\")).toPandas()\n",
    "# val = spark.read.parquet(os.path.join(model_data_path, \"val_set.parquet\")).toPandas()\n",
    "# test = spark.read.parquet(os.path.join(model_data_path, \"test_set.parquet\")).toPandas()\n",
    "\n",
    "# train.repartition(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define PySpark ALS model\n",
    "# als = ALS(\n",
    "#     userCol=\"user_index\",\n",
    "#     itemCol=\"bus_index\",\n",
    "#     ratingCol=\"rating\",\n",
    "#     coldStartStrategy=\"drop\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # # Grid search through hyperparameters\n",
    "# # paramGrid = (ParamGridBuilder()\n",
    "# #              .addGrid(als.rank, [5, 10, 15])\n",
    "# #              .addGrid(als.maxIter, [5, 10, 20])\n",
    "# #              .addGrid(als.regParam, [0.01, 0.1, 0.5])\n",
    "# #              .build())\n",
    "\n",
    "# # define criterion\n",
    "# evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "# # define cross-validation w simple grid\n",
    "# crossval = CrossValidator(\n",
    "#     estimator=als,\n",
    "#     evaluator=evaluator,\n",
    "#     estimatorParamMaps=paramGrid,\n",
    "#     numFolds=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = train.rdd\n",
    "# partitions = rdd.glom().collect()\n",
    "# for index, partition in enumerate(partitions):\n",
    "#     print(f\"Partition {index} contains {len(partition)} rows.\")\n",
    "#     if len(partition) > 0:\n",
    "#         print(f\"Sample data from partition {index}: {partition[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# open log \n",
    "# mlflow.set_experiment(\"ALS_Hyperparameter_Tuning\")\n",
    "\n",
    "# log results\n",
    "# with mlflow.start_run():\n",
    "    \n",
    "    # fit model using cross-validation\n",
    "    # cv_model = crossval.fit(train)\n",
    "    \n",
    "    # Log the best model\n",
    "    # best_model = cv_model.bestModel\n",
    "    # mlflow.spark.log_model(best_model, \"best_model\")\n",
    "    \n",
    "    # # Log metrics and model parameters for each parameter combination\n",
    "    # for param_map, metric in zip(crossval.getEstimatorParamMaps(), cv_model.avgMetrics):\n",
    "    #     rank = param_map[als.rank]\n",
    "    #     regParam = param_map[als.regParam]\n",
    "    #     maxIter = param_map[als.maxIter]\n",
    "        \n",
    "    #     mlflow.log_param(\"rank\", rank)\n",
    "    #     mlflow.log_param(\"regParam\", regParam)\n",
    "    #     mlflow.log_param(\"maxIter\", maxIter)\n",
    "    #     mlflow.log_metric(\"validation_rmse\", metric)\n",
    "\n",
    "    # # Log validation scores\n",
    "    # validation_predictions = best_model.transform(val)\n",
    "    # validation_rmse = evaluator.evaluate(validation_predictions)\n",
    "    # mlflow.log_metric(\"validation_rmse\", validation_rmse)\n",
    "\n",
    "\n",
    "# # Log test metrics (optional, after final model selection)\n",
    "# test_predictions = best_model.transform(test)\n",
    "# test_rmse = evaluator.evaluate(test_predictions)\n",
    "# mlflow.log_metric(\"test_rmse\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define paths\n",
    "# model_data_path = \"../data/interim/\"\n",
    "\n",
    "# # read data and conver to pandas df\n",
    "# train = spark.read.parquet(os.path.join(model_data_path, \"train_set.parquet\")).toPandas()\n",
    "# val = spark.read.parquet(os.path.join(model_data_path, \"val_set.parquet\")).toPandas()\n",
    "# test = spark.read.parquet(os.path.join(model_data_path, \"test_set.parquet\")).toPandas()\n",
    "\n",
    "# def convert_to_tensors(df):\n",
    "#     ratings_tensor = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "#     user_id_tensor = torch.tensor(df['user_index'].values, dtype=torch.int64)\n",
    "#     bus_id_tensor = torch.tensor(df['bus_index'].values, dtype=torch.int64)\n",
    "#     return ratings_tensor, user_id_tensor, bus_id_tensor\n",
    "\n",
    "# def save_tensors(prefix, **tensors):\n",
    "#     for name, tensor in tensors.items():\n",
    "#         file_path = os.path.join(model_data_path, f'{prefix}_{name}.pt')\n",
    "#         torch.save(tensor, file_path)\n",
    "\n",
    "# # Convert data to tensors\n",
    "# train_tensors = convert_to_tensors(train)\n",
    "# val_tensors = convert_to_tensors(val)\n",
    "# test_tensors = convert_to_tensors(test)\n",
    "\n",
    "# # Save tensors\n",
    "# save_tensors('train', ratings=train_tensors[0], user_id=train_tensors[1], bus_id=train_tensors[2])\n",
    "# save_tensors('val', ratings=val_tensors[0], user_id=val_tensors[1], bus_id=val_tensors[2])\n",
    "# save_tensors('test', ratings=test_tensors[0], user_id=test_tensors[1], bus_id=test_tensors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_data_path = \"../data/interim/\"\n",
    "\n",
    "data_path = mac_data_path\n",
    "\n",
    "pd.read_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "mac_data_path = \"../data/interim/\"\n",
    "\n",
    "data_path = mac_data_path\n",
    "\n",
    "# # Check if Mac GPU is available\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Check if nvidia gpu is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "def load_tensors(directory, prefix, device):\n",
    "    ratings = torch.load(os.path.join(directory, f'{prefix}_ratings.pt')).to(device)\n",
    "    user_id = torch.load(os.path.join(directory, f'{prefix}_user_id.pt')).to(device)\n",
    "    bus_id = torch.load(os.path.join(directory, f'{prefix}_bus_id.pt')).to(device)\n",
    "    return ratings, user_id, bus_id\n",
    "\n",
    "# Load tensors and move them to the device\n",
    "train_tensors = load_tensors(data_path, 'train', device)\n",
    "val_tensors = load_tensors(data_path, 'val', device)\n",
    "test_tensors = load_tensors(data_path, 'test', device)\n",
    "\n",
    "# unpack \n",
    "train_ratings, train_user_id, train_bus_id = train_tensors\n",
    "val_ratings, val_user_id, val_bus_id = val_tensors\n",
    "test_ratings, test_user_id, test_bus_id = test_tensors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 287116\n",
      "Number of unique business's : 113502\n"
     ]
    }
   ],
   "source": [
    "# Concat user and business data into one tensor\n",
    "total_user_ids = torch.concat((train_user_id, val_user_id, test_user_id), dim=0)\n",
    "total_business_ids = torch.concat((train_bus_id, val_bus_id, test_bus_id))\n",
    "\n",
    "# get number of unique entities\n",
    "num_unique_users = torch.unique(total_user_ids).shape[0] \n",
    "num_unique_bus = torch.unique(total_business_ids).shape[0]\n",
    "\n",
    "# output findings\n",
    "print(f\"Number of unique users: {num_unique_users}\")\n",
    "print(f\"Number of unique business's : {num_unique_bus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, business_data, user_data, ratings_data, transform=None) -> None:\n",
    "        self.business_data = business_data\n",
    "        self.user_data = user_data\n",
    "        self.ratings_data = ratings_data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        business = self.business_data[index]\n",
    "        user = self.user_data[index]\n",
    "        rating = self.ratings_data[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            business, user, rating = self.transform(business, user, rating)\n",
    "        \n",
    "        return business, user, rating\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "\n",
    "# initially starting with just cosine similiarity\n",
    "class MatrixFact(nn.Module):\n",
    "    \n",
    "    def __init__ (self, rank, num_users, num_bus):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.user_emb = nn.Embedding(num_users, rank)\n",
    "        self.bus_emb = nn.Embedding(num_bus, rank)\n",
    "        self.fc1 = nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self, user_id, business_id):\n",
    "        # get entity ids\n",
    "        user_emb = self.user_emb(user_id)     \n",
    "        bus_emb = self.bus_emb(business_id) \n",
    "        \n",
    "        \n",
    "        # # calulcate similarities\n",
    "        product = user_emb * bus_emb # element wise / essentially dot product of sparse matrix \n",
    "        cos_sim = product.sum(dim=1, keepdim=True)\n",
    "        result = self.fc1(cos_sim)\n",
    "        \n",
    "        return result.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First iteration of the model, I will be just using cosine similiarity and storing results in mlflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "rank = 10\n",
    "batch_size = 1000\n",
    "num_epochs = 3000\n",
    "# # create data \n",
    "# users = torch.randint(num_users, (data_size,), requires_grad=False)\n",
    "# business = torch.randint(num_business, (data_size,), requires_grad=False)\n",
    "# ratings = torch.randint(low=1, high=5, size=(data_size,), requires_grad=False).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and define optimization\n",
    "\n",
    "model = MatrixFact(rank, num_unique_users, num_unique_bus)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_data = CustomDataset(train_bus_id, train_user_id, train_ratings)\n",
    "train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),    # Parameters of the model to optimize\n",
    "    lr=0.001,              # Learning rate (default is 0.001)\n",
    "    betas=(0.9, 0.999),    # Coefficients for computing running averages of gradient and its square\n",
    "    eps=1e-08,             # Term added to the denominator to improve numerical stability\n",
    "    weight_decay=.01       # Weight decay (L2 penalty)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  ['train_user_id': long (required), 'train_bus_id': long (required)]\n",
       "outputs: \n",
       "  [Tensor('float64', (-1,))]\n",
       "params: \n",
       "  None"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "# Set an experiment by name\n",
    "mlflow.set_experiment(\"CF Initial Experiment\")\n",
    "\n",
    "# Example input data for signature\n",
    "input_data = pd.DataFrame({\n",
    "    'train_user_id': np.array([1]),  # Example user ID\n",
    "    'train_bus_id': np.array([1])    # Example business ID\n",
    "})\n",
    "\n",
    "# Example model output (based on a dummy forward pass)\n",
    "output_data = np.array([0.5])  # Assuming your model outputs a float (e.g., a probability)\n",
    "\n",
    "# Infer the signature\n",
    "signature = infer_signature(input_data, output_data)\n",
    "# # Convert the tensors to numpy arrays for logging purposes\n",
    "# input_example = pd.DataFrame({\n",
    "#     'train_user_id': input_example_torch['train_user_id'].numpy(),\n",
    "#     'train_bus_id': input_example_torch['train_bus_id'].numpy()\n",
    "# })\n",
    "\n",
    "signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, MSE Loss: 17.80499267578125\n",
      "Epoch 100, MSE Loss: 16.903764724731445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, train_ratings)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# backward pass and optimization\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# log loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine Learning Projects/Business Recommendation System/brs/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Machine Learning Projects/Business Recommendation System/brs/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Machine Learning Projects/Business Recommendation System/brs/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start a new MLflow run\n",
    "with mlflow.start_run():\n",
    "    # log hyperparameters\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"learning_rate\", optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # log model architecture\n",
    "    model_architecture = str(model)\n",
    "    with open(\"model_architecture.txt\", \"w\") as f:\n",
    "        f.write(model_architecture)\n",
    "    mlflow.log_artifact(\"model_architecture.txt\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        pred = model(train_user_id, train_bus_id)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = criterion(pred, train_ratings)\n",
    "        \n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # log loss\n",
    "        mlflow.log_metric(\"loss\", loss.item(), step=epoch)\n",
    "        \n",
    "        if loss.item() < 1:\n",
    "            break\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, MSE Loss: {loss.item()}')\n",
    "    \n",
    "    # log the model\n",
    "    mlflow.pytorch.log_model(\n",
    "        model, \n",
    "        \"model\", \n",
    "        input_example=input_example\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
