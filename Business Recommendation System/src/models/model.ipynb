{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV5dUh1uK4YU",
        "outputId": "fda8d837-9f3d-4151-edae-9586baff3639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/12.8 MB\u001b[0m \u001b[31m204.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m192.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m192.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.26.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# !pip install langdetect\n",
        "!pip install wandb -qU\n",
        "# !pip install fasttext\n",
        "!pip install h5py\n",
        "!pip install gensim\n",
        "\n",
        "# Standard library imports\n",
        "# from langdetect import detect, detect_langs\n",
        "import os\n",
        "import warnings\n",
        "import h5py\n",
        "import gc\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Data processing and numerical libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import scipy.sparse as sp\n",
        "# import dask.dataframe as dd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning and recommendation libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import wandb\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "# import fasttext\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "from transformers import BertPreTrainedModel\n",
        "from transformers import BertModel, BertTokenizer\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "# from pyspark.ml.recommendation import ALS\n",
        "# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "# from pyspark.ml.evaluation import RegressionEvaluator\n",
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.sql.functions import col, udf\n",
        "# from pyspark.sql.types import StringType\n",
        "# from pyspark.sql.types import IntegerType, FloatType, StringType\n",
        "# from pyspark.ml import Pipeline\n",
        "\n",
        "# MLflow for experiment tracking\n",
        "# import mlflow\n",
        "# from pyngrok import ngrok\n",
        "\n",
        "# IPython for displaying outputs\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "OYPTagoZMPul",
        "outputId": "c25957f1-aab3-412e-d8e2-20dd620f3bd7"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzFcaOO5RHkc"
      },
      "outputs": [],
      "source": [
        "# Convert df to structured array\n",
        "# RECAP: sturctured arrays are more effieicnt cause they store information in continguous memory\n",
        "\n",
        "# yelp_df = pd.read_parquet('/content/drive/MyDrive/Machine Learning/BRS/final_df.parquet')\n",
        "\n",
        "# Define the structured array dtype\n",
        "# dtype = [\n",
        "#     ('user_num_id', 'i4'),\n",
        "#     ('business_num_id', 'i4'),\n",
        "#     ('log_business_review_count_norm', 'f4'),\n",
        "#     ('bus_avg_rating_norm', 'f4'),\n",
        "#     ('region_code', 'i4'),\n",
        "#     ('state_code', 'i4'),\n",
        "#     ('city_code', 'i4'),\n",
        "#     ('day_of_year', 'i4'),\n",
        "#     ('day_of_week', 'i4'),\n",
        "#     ('mean_centered_rating', 'f4'),\n",
        "#     ('u_pca1', 'f4'),\n",
        "#     ('u_pca2', 'f4'),\n",
        "#     ('u_pca3', 'f4'),\n",
        "# ]\n",
        "\n",
        "# # Convert the DataFrame to a structured array\n",
        "# structured_array = np.array(list(yelp_df.to_records(index=False)), dtype=dtype)\n",
        "\n",
        "# # Save the structured array as a memory-mapped file\n",
        "# np.save('/content/drive/MyDrive/Machine Learning/BRS/Numpy Arrays/yelp_structured.npy', structured_array,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lFy-iO3qklW",
        "outputId": "a2840e16-8c00-4033-90c1-89f4d4e170c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# class Lazy_CFDataset(Dataset):\n",
        "#     def __init__(self, yelp_file_path, categories_file_path, token_file_path, att_file_path):\n",
        "#         # Load memory-mapped structured array\n",
        "#         try:\n",
        "#             # Load the structured array\n",
        "#             self.yelp_data = np.load(yelp_file_path, mmap_mode='r')  # Open in read-only mode\n",
        "\n",
        "#             # Load other file paths\n",
        "#             self.categories_file_path = categories_file_path\n",
        "#             self.token_file_path = token_file_path\n",
        "#             self.att_file_path = att_file_path\n",
        "\n",
        "#             # Pre-compute the dataset length\n",
        "#             self.length = len(self.yelp_data)\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error loading data: {e}\")\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.length\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Load the tensors when accessing items\n",
        "#         user_id = torch.tensor(self.yelp_data['user_num_id'][idx], dtype=torch.int32)  # Shape: (N,)\n",
        "#         business_id = torch.tensor(self.yelp_data['business_num_id'][idx], dtype=torch.int32)  # Shape: (N,)\n",
        "#         region_id = torch.tensor(self.yelp_data['region_code'][idx], dtype=torch.int32)  # Shape: (N,)\n",
        "#         dotw = torch.tensor(self.yelp_data['day_of_week'][idx], dtype=torch.int32)  # Shape: (N,)\n",
        "#         doty = torch.tensor(self.yelp_data['day_of_year'][idx], dtype=torch.int32)  # Shape: (N,)\n",
        "#         mean_centered_rating = torch.tensor(self.yelp_data['mean_centered_rating'][idx], dtype=torch.float32)  # Shape: (N,)\n",
        "\n",
        "#         # Group PCA features into a single tensor of shape (3,)\n",
        "#         pca_features = torch.tensor([\n",
        "#             self.yelp_data['u_pca1'][idx],\n",
        "#             self.yelp_data['u_pca2'][idx],\n",
        "#             self.yelp_data['u_pca3'][idx]\n",
        "#         ], dtype=torch.float32)  # Shape: (3,)\n",
        "\n",
        "#         # Load other tensors\n",
        "#         tokens = torch.load(self.token_file_path)[idx]  # Shape: (Token_Length,)\n",
        "#         att_mask = torch.load(self.att_file_path)[idx]  # Shape: (Token_Length,)\n",
        "#         category = torch.load(self.categories_file_path)[idx]  # Shape: (N,)\n",
        "\n",
        "#         return {\n",
        "#             'user_id': user_id,\n",
        "#             'business_id': business_id,\n",
        "#             'region_id': region_id,\n",
        "#             'dotw': dotw,\n",
        "#             'doty': doty,\n",
        "#             'pca_features': pca_features,  # (3,)\n",
        "#             'tokens': tokens,\n",
        "#             'att_mask': att_mask,\n",
        "#             'category': category\n",
        "#         }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IetSW714kNWT"
      },
      "outputs": [],
      "source": [
        "class CFDataset(Dataset):\n",
        "    def __init__(self, yelp_file_path, categories_file_path, token_file_path, mem_map=False):\n",
        "        # Load memory-mapped structured array\n",
        "        try:\n",
        "            # Load the structured array\n",
        "            if mem_map:\n",
        "              yelp_data = np.load(yelp_file_path, mmap_mode='r')\n",
        "            else:\n",
        "              yelp_data = np.load(yelp_file_path)\n",
        "\n",
        "            # Extract features from the structured array\n",
        "            self.user_id = torch.tensor(yelp_data['user_num_id'], dtype=torch.int32)  # Shape: (N,)\n",
        "            self.business_id = torch.tensor(yelp_data['business_num_id'], dtype=torch.int32)  # Shape: (N,)\n",
        "            self.region_id = torch.tensor(yelp_data['region_code'], dtype=torch.int32)  # Shape: (N,)\n",
        "            self.dotw = torch.tensor(yelp_data['day_of_week'], dtype=torch.int32)  # Shape: (N,)\n",
        "            self.doty = torch.tensor(yelp_data['day_of_year'], dtype=torch.int32)  # Shape: (N,)\n",
        "            self.mean_centered_rating = torch.tensor(yelp_data['mean_centered_rating'], dtype=torch.float32)  # Shape: (N,)\n",
        "\n",
        "            # # Group PCA features into a single tensor of shape (N, 3)\n",
        "            self.pca_features = torch.stack([\n",
        "                torch.tensor(yelp_data['u_pca1'], dtype=torch.float32),\n",
        "                torch.tensor(yelp_data['u_pca2'], dtype=torch.float32),\n",
        "                torch.tensor(yelp_data['u_pca3'], dtype=torch.float32)\n",
        "            ], dim=1)  # Shape: (N, 3)\n",
        "\n",
        "            # Load other memory-mapped tensors\n",
        "            self.token_emb = torch.load(token_file_path)  # Shape: (N, Token_Length)\n",
        "            self.categories = torch.load(categories_file_path)  # Shape: (N,)\n",
        "\n",
        "            # Pre-compute the dataset length based on tokens\n",
        "            self.length = len(self.token_emb)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get a batch of data\n",
        "        user_id = self.user_id[idx]\n",
        "        business_id = self.business_id[idx]\n",
        "        region_id = self.region_id[idx]\n",
        "        dotw = self.dotw[idx]\n",
        "        doty = self.doty[idx]\n",
        "        pca_features = self.pca_features[idx]  # (3,)\n",
        "        token_emb = self.token_emb[idx]\n",
        "        category = self.categories[idx].to(torch.long)\n",
        "\n",
        "        return {\n",
        "            'user_id': user_id,\n",
        "            'business_id': business_id,\n",
        "            'region_id': region_id,\n",
        "            'dotw': dotw,\n",
        "            'doty': doty,\n",
        "            'pca_features': pca_features,  # (3,)\n",
        "            'tokens': token_emb,\n",
        "            'category': category\n",
        "        }, self.mean_centered_rating[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HsIQ2h_WZuC",
        "outputId": "73f19e97-4fc7-4971-d8e1-50558a2edb44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training and validation datasets loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def gather_and_load_data(rem_yelp_file_path, rem_cat_file_path, rem_token_file_path, save_dir):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load structured array and tensors from drive\n",
        "    yelp_array = np.load(rem_yelp_file_path, allow_pickle=True)  # Allow_pickle is needed for structured arrays!!\n",
        "    token_emb = torch.load(rem_token_file_path)\n",
        "    categories = torch.load(rem_cat_file_path)\n",
        "\n",
        "    curr_time = time.time()\n",
        "    print(f\"Data has been loaded from Google Drive\")\n",
        "    print(f\"Read time from Google Drive: {curr_time - start_time:.4f} seconds\")\n",
        "\n",
        "    # Prepare the data for splitting\n",
        "    num_samples = yelp_array.shape[0] \n",
        "    indices = np.arange(num_samples)\n",
        "\n",
        "    # Split data \n",
        "    train_indices, temp_indices = train_test_split(indices, test_size=0.2, random_state=42)  \n",
        "    valid_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42) \n",
        "\n",
        "    # Create datasets \n",
        "    train_yelp_data = yelp_array[train_indices]\n",
        "    valid_yelp_data = yelp_array[valid_indices]\n",
        "    test_yelp_data = yelp_array[test_indices]\n",
        "\n",
        "    train_token_emb = token_emb[train_indices]\n",
        "    valid_token_emb = token_emb[valid_indices]\n",
        "    test_token_emb = token_emb[test_indices]\n",
        "\n",
        "\n",
        "    train_categories = categories[train_indices]\n",
        "    valid_categories = categories[valid_indices]\n",
        "    test_categories = categories[test_indices]\n",
        "\n",
        "    # Save the split datasets remotely\n",
        "    np.save(f'{save_dir}/train_yelp.npy', train_yelp_data)\n",
        "    np.save(f'{save_dir}/valid_yelp.npy', valid_yelp_data)\n",
        "    np.save(f'{save_dir}/test_yelp.npy', test_yelp_data)\n",
        "\n",
        "\n",
        "    torch.save(train_token_emb, f'{save_dir}/train_tok_emb.pt')\n",
        "    torch.save(valid_token_emb, f'{save_dir}/valid_tok_emb.pt')\n",
        "    torch.save(test_token_emb, f'{save_dir}/test_tok_emb.pt')\n",
        "\n",
        "    torch.save(train_categories, f'{save_dir}/train_categories.pt')\n",
        "    torch.save(valid_categories, f'{save_dir}/valid_categories.pt')\n",
        "    torch.save(test_categories, f'{save_dir}/test_categories.pt')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Data has been written to Google Drive\")\n",
        "    print(f\"Write time from Google Drive: {end_time - curr_time:.4f} seconds\")\n",
        "\n",
        "    # Remove the original versions from memory\n",
        "    del yelp_array, token_emb, categories\n",
        "\n",
        "    # Run garbage collection to free up memory\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"Dataset has been split into training, validation, and test sets and saved remotely.\")\n",
        "\n",
        "\n",
        "\n",
        "def create_datasets_from_saved_files(split_dir, dataset_save_dir):\n",
        "\n",
        "    # Ensure the dataset save directory exists\n",
        "    os.makedirs(dataset_save_dir, exist_ok=True)\n",
        "\n",
        "    # Load the saved data files\n",
        "    train_yelp_data_path = f'{split_dir}/train_yelp.npy'\n",
        "    valid_yelp_data_path = f'{split_dir}/valid_yelp.npy'\n",
        "    test_yelp_data_path = f'{split_dir}/test_yelp.npy'\n",
        "\n",
        "    train_token_emb_path = f'{split_dir}/train_tok_emb.pt'\n",
        "    valid_token_emb_path = f'{split_dir}/valid_tok_emb.pt'\n",
        "    test_token_emb_path = f'{split_dir}/test_tok_emb.pt'\n",
        "\n",
        "    train_categories_path = f'{split_dir}/train_categories.pt'\n",
        "    valid_categories_path = f'{split_dir}/valid_categories.pt'\n",
        "    test_categories_path = f'{split_dir}/test_categories.pt'\n",
        "\n",
        "\n",
        "    # Create Dataset objects\n",
        "    train_dataset = CFDataset(train_yelp_data_path, train_categories_path, train_token_emb_path)\n",
        "    valid_dataset = CFDataset(valid_yelp_data_path, valid_categories_path, valid_token_emb_path)\n",
        "    test_dataset = CFDataset(test_yelp_data_path, test_categories_path, test_token_emb_path)\n",
        "\n",
        "    # Save the Dataset objects\n",
        "    torch.save(train_dataset, f'{dataset_save_dir}/train_dataset.pt')\n",
        "    torch.save(valid_dataset, f'{dataset_save_dir}/valid_dataset.pt')\n",
        "    torch.save(test_dataset, f'{dataset_save_dir}/test_dataset.pt')\n",
        "\n",
        "    print(\"Datasets have been created and saved to:\", dataset_save_dir)\n",
        "\n",
        "\n",
        "\n",
        "def load_datasets(dataset_dir, load_test=False):\n",
        "\n",
        "    datasets = {}\n",
        "\n",
        "    # Load training and validation datasets\n",
        "    try:\n",
        "        datasets['train'] = torch.load(os.path.join(dataset_dir, 'train_dataset.pt'))\n",
        "        datasets['valid'] = torch.load(os.path.join(dataset_dir, 'valid_dataset.pt'))\n",
        "        print(\"Training and validation datasets loaded successfully.\")\n",
        "\n",
        "        if load_test:\n",
        "            datasets['test'] = torch.load(os.path.join(dataset_dir, 'test_dataset.pt'))\n",
        "            print(\"Test dataset loaded successfully.\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading datasets: {e}\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Local Raw Data file paths\n",
        "yelp_local_path = '/content/yelp_local.npy'\n",
        "train_categories_path = '/content/train_categories.pt'\n",
        "train_tok_emb_path = '/content/drive/MyDrive/Machine Learning/BRS/embeddings_tensor.pt'\n",
        "cf_dataset_path = '/content/cf_dataset.pt'\n",
        "\n",
        "# Remote Raw Data file paths\n",
        "rem_yelp_file_path = '/content/drive/MyDrive/Machine Learning/BRS/Numpy Arrays/yelp_structured.npy'\n",
        "rem_token_file_path = '/content/drive/MyDrive/Machine Learning/BRS/embeddings_tensor.pt'\n",
        "rem_cat_file_path = '/content/drive/MyDrive/Machine Learning/BRS/categories_tensor.pt'\n",
        "\n",
        "# Split tensor directory\n",
        "split_dir = '/content/drive/MyDrive/Machine Learning/BRS/Split Data'\n",
        "\n",
        "# Final dataset directory, constructed CF datasets\n",
        "final_datesets_dir = '/content/drive/MyDrive/Machine Learning/BRS/Final Datasets'\n",
        "\n",
        "# Create Dataset and save to directory\n",
        "\n",
        "\n",
        "# gather_and_load_data(rem_yelp_file_path, rem_cat_file_path, rem_token_file_path, split_dir)\n",
        "\n",
        "# create_datasets_from_saved_files(split_dir, final_datesets_dir)\n",
        "\n",
        "\n",
        "# # # Load datasets\n",
        "datasets = load_datasets(final_datesets_dir)  # Load only train and valid datasets\n",
        "\n",
        "# # # Unpack the datasets\n",
        "train_dataset = datasets['train']\n",
        "val_dataset = datasets['valid']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTrr1L59QblX",
        "outputId": "6674cc11-d10a-4cc9-8f4f-9a99ae86ff33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([768])"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0][0]['tokens'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfS_xKs2RFuC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HybridCFModel(nn.Module):\n",
        "    def __init__(self, vocab_size=30522, rank=32, num_users=287116, num_bus=148523, num_regions=11,\n",
        "                 num_dotw=7, num_doty=367, num_pca=3, category_size=1311, token_length=768, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.token_length = token_length\n",
        "\n",
        "        # Ensures rank is divisible by num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_dim = (rank // num_heads) * num_heads\n",
        "\n",
        "        # Embeddings categorical features\n",
        "        self.user_emb = nn.Embedding(num_users, rank)\n",
        "        self.bus_emb = nn.Embedding(num_bus, rank)\n",
        "        self.region_emb = nn.Embedding(num_regions, rank)\n",
        "        self.dotw_emb = nn.Embedding(num_dotw, rank)\n",
        "        self.doty_emb = nn.Embedding(num_doty, rank)\n",
        "        self.categories = nn.EmbeddingBag(category_size, rank, mode='mean', padding_idx=0)\n",
        "\n",
        "        # Layer for numerical PCA features - transforms to rank dimension\n",
        "        self.pca_layer = nn.Sequential(\n",
        "            nn.Linear(num_pca, rank),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(rank)\n",
        "        )\n",
        "\n",
        "        # Token embedding layer\n",
        "        self.token_layer = nn.Sequential(\n",
        "            nn.Linear(token_length, rank),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(rank)\n",
        "        )\n",
        "\n",
        "        # Projection layers for multi-head attention\n",
        "        self.user_proj = nn.Linear(rank, self.attention_dim)\n",
        "        self.bus_proj = nn.Linear(rank, self.attention_dim)\n",
        "\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=self.attention_dim, num_heads=num_heads)\n",
        "\n",
        "        total_input_size = rank * 8 + self.attention_dim\n",
        "\n",
        "        # Final layers for the combined CF and CBF features\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(total_input_size, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        # CF features\n",
        "        user_emb = self.user_emb(kwargs['user_id'])  \n",
        "        bus_emb = self.bus_emb(kwargs['business_id'])  \n",
        "        region_emb = self.region_emb(kwargs['region_id']) \n",
        "        dotw_emb = self.dotw_emb(kwargs['dotw'])  \n",
        "        doty_emb = self.doty_emb(kwargs['doty']) \n",
        "        cat_emb = self.categories(kwargs['category']) \n",
        "\n",
        "        # Content-based filtering features\n",
        "        pca_features = self.pca_layer(kwargs['pca_features']) \n",
        "        tokens_emb = self.token_layer(kwargs['tokens'])\n",
        "\n",
        "        # Project embeddings to attention dimension\n",
        "        user_proj = self.user_proj(user_emb)\n",
        "        bus_proj = self.bus_proj(bus_emb)\n",
        "\n",
        "        # Multi-head attention for user-business interaction\n",
        "        user_bus_interaction, _ = self.multihead_attn(user_proj.unsqueeze(0), bus_proj.unsqueeze(0), bus_proj.unsqueeze(0))\n",
        "        user_bus_interaction = user_bus_interaction.squeeze(0)\n",
        "\n",
        "        # Combine CF and CBF features\n",
        "        combined_features = torch.cat([\n",
        "            user_emb, bus_emb, user_bus_interaction, region_emb, dotw_emb, doty_emb, cat_emb, pca_features, tokens_emb\n",
        "        ], dim=1)\n",
        "\n",
        "        # Final prediction\n",
        "        result = self.fc_layers(combined_features)\n",
        "\n",
        "        return result.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG0jN6niRHTP",
        "outputId": "221ff666-c3d7-4ddf-b6db-22302d61962c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device : cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "HybridCFModel(\n",
              "  (user_emb): Embedding(287116, 10)\n",
              "  (bus_emb): Embedding(148523, 10)\n",
              "  (region_emb): Embedding(11, 10)\n",
              "  (dotw_emb): Embedding(7, 10)\n",
              "  (doty_emb): Embedding(367, 10)\n",
              "  (categories): EmbeddingBag(1311, 10, mode='mean', padding_idx=0)\n",
              "  (pca_layer): Sequential(\n",
              "    (0): Linear(in_features=3, out_features=10, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (token_layer): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=10, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (user_proj): Linear(in_features=10, out_features=8, bias=True)\n",
              "  (bus_proj): Linear(in_features=10, out_features=8, bias=True)\n",
              "  (multihead_attn): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
              "  )\n",
              "  (fc_layers): Sequential(\n",
              "    (0): Linear(in_features=88, out_features=256, bias=True)\n",
              "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.2, inplace=False)\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    (6): ReLU()\n",
              "    (7): Dropout(p=0.2, inplace=False)\n",
              "    (8): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# # Hyperparameters\n",
        "rank = 16\n",
        "\n",
        "\n",
        "# Model / Optimizer Information\n",
        "batch_size = 2000\n",
        "lr = .001\n",
        "num_epochs = 15\n",
        "betas = (0.9, 0.999)\n",
        "eps=1e-08\n",
        "weight_decay=.01 #L2 normalization\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "# Example with optimizations\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=6,         # Adjust based on your system\n",
        "    pin_memory=True        # Accelerates transfer to GPU\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=6,         # Adjust based on your system\n",
        "    pin_memory=True        # Accelerates transfer to GPU\n",
        ")\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# init device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device : {device}\")\n",
        "# Initialize Model\n",
        "model = HybridCFModel(rank=rank)\n",
        "model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjcxpqNWRIYR"
      },
      "outputs": [],
      "source": [
        "# # initialize model and define optimization\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),     # Parameters of the model to optimize\n",
        "    lr=lr,                 # Learning rate (default is 0.001)\n",
        "    betas=betas,           # Coefficients for computing running averages of gradient and its square\n",
        "    eps=eps,               # Term added to the denominator to improve numerical stability\n",
        "    weight_decay=weight_decay  # Weight decay (L2 penalty) applied directly\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889,
          "referenced_widgets": [
            "45c25f995dca46289974d1f9615da168",
            "fba069b5d9694969b0cc640a5be0899d",
            "bf5027ff25644f3785924adc5262f770",
            "f8e67d727d9a4435b67ff0b168ef1cf9",
            "b0c51219efcb4f7992f2f9757740c24e",
            "c658ab4fcc6c4666ae4b561c7a96bc80",
            "00ff49daed4b4512b7a910bb36f8ecfc",
            "e476b4dd821844cd9422ab7fb42bc603"
          ]
        },
        "id": "zTdYdBYLYn1y",
        "outputId": "56c3ab5e-f917-4a30-ab6e-7b1391867fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:zol5h2hi) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45c25f995dca46289974d1f9615da168",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.013 MB of 0.013 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">visionary-snowflake-24</strong> at: <a href='https://wandb.ai/teddybytesorg/Updated%20Model%20with%20Bert%20Tokenization/runs/zol5h2hi' target=\"_blank\">https://wandb.ai/teddybytesorg/Updated%20Model%20with%20Bert%20Tokenization/runs/zol5h2hi</a><br/> View project at: <a href='https://wandb.ai/teddybytesorg/Updated%20Model%20with%20Bert%20Tokenization' target=\"_blank\">https://wandb.ai/teddybytesorg/Updated%20Model%20with%20Bert%20Tokenization</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240930_101553-zol5h2hi/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:zol5h2hi). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240930_101633-ibzi8all</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/teddybytesorg/Updated%20Model%20with%20Bert%20Tokenization/runs/ibzi8all' target=\"_blank\">proud-yogurt-25</a></strong> to <a href='https://wandb.ai/teddybytesorg/Updated%20Model%20with%20Bert%20Tokenization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/teddybytesorg/Updated%20Model%20with%20Bert%20Tokenization' target=\"_blank\">https://wandb.ai/teddybytesorg/Updated%20Model%20with%20Bert%20Tokenization</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/teddybytesorg/Updated%20Model%20with%20Bert%20Tokenization/runs/ibzi8all' target=\"_blank\">https://wandb.ai/teddybytesorg/Updated%20Model%20with%20Bert%20Tokenization/runs/ibzi8all</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HybridCFModel(\n",
            "  (user_emb): Embedding(287116, 10)\n",
            "  (bus_emb): Embedding(148523, 10)\n",
            "  (region_emb): Embedding(11, 10)\n",
            "  (dotw_emb): Embedding(7, 10)\n",
            "  (doty_emb): Embedding(367, 10)\n",
            "  (categories): EmbeddingBag(1311, 10, mode='mean', padding_idx=0)\n",
            "  (pca_layer): Sequential(\n",
            "    (0): Linear(in_features=3, out_features=10, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (token_layer): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=10, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (user_proj): Linear(in_features=10, out_features=8, bias=True)\n",
            "  (bus_proj): Linear(in_features=10, out_features=8, bias=True)\n",
            "  (multihead_attn): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
            "  )\n",
            "  (fc_layers): Sequential(\n",
            "    (0): Linear(in_features=88, out_features=256, bias=True)\n",
            "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.2, inplace=False)\n",
            "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (6): ReLU()\n",
            "    (7): Dropout(p=0.2, inplace=False)\n",
            "    (8): Linear(in_features=128, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "def to_device(data, device):\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    elif isinstance(data, dict):\n",
        "        return {key: to_device(val, device) for key, val in data.items()}\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# watch -n 1 nvidia-smi\n",
        "\n",
        "# Initialize W&B to watch the model\n",
        "wandb.init(project=\"Updated Model with Bert Tokenization\")\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "\n",
        "\n",
        "# Log model architecture using wandb.config\n",
        "wandb.config.update({\n",
        "    \"descr\": \"Even better model?\",\n",
        "    \"learning_rate\": lr,\n",
        "    \"architecture\": \"CF\",\n",
        "    \"dataset\": \"Yelp\",\n",
        "    \"betas\": betas,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"rank\": rank,\n",
        "    \"weight_decay\": weight_decay,\n",
        "    \"model_architecture\": print(model)\n",
        "})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "6lOLD9seRJk-",
        "outputId": "3e8e5887-f3b9-4658-ed41-508d5280dbcb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15: 100%|██████████| 1716/1716 [00:55<00:00, 30.70batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Iter: 1715, Train Loss: 0.0400, Val Loss: 0.5308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/15: 100%|██████████| 1716/1716 [00:56<00:00, 30.53batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2, Iter: 1715, Train Loss: 0.4912, Val Loss: 0.6325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/15: 100%|██████████| 1716/1716 [00:55<00:00, 31.13batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3, Iter: 1715, Train Loss: 0.3124, Val Loss: 0.4996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/15: 100%|██████████| 1716/1716 [00:55<00:00, 31.00batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4, Iter: 1715, Train Loss: 0.3360, Val Loss: 0.5549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/15: 100%|██████████| 1716/1716 [00:55<00:00, 31.08batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5, Iter: 1715, Train Loss: 0.6895, Val Loss: 0.7997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/15: 100%|██████████| 1716/1716 [00:55<00:00, 30.72batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6, Iter: 1715, Train Loss: 0.3546, Val Loss: 0.4871\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/15: 100%|██████████| 1716/1716 [00:55<00:00, 31.18batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7, Iter: 1715, Train Loss: 0.0705, Val Loss: 0.4793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/15: 100%|██████████| 1716/1716 [00:55<00:00, 30.86batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8, Iter: 1715, Train Loss: 0.2678, Val Loss: 0.4802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/15: 100%|██████████| 1716/1716 [00:55<00:00, 30.80batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9, Iter: 1715, Train Loss: 0.4490, Val Loss: 0.4862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/15: 100%|██████████| 1716/1716 [00:55<00:00, 30.77batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10, Iter: 1715, Train Loss: 0.1713, Val Loss: 0.4779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/15: 100%|██████████| 1716/1716 [00:54<00:00, 31.41batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11, Iter: 1715, Train Loss: 0.0525, Val Loss: 0.4779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/15: 100%|██████████| 1716/1716 [00:55<00:00, 30.98batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12, Iter: 1715, Train Loss: 0.1448, Val Loss: 0.4779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/15: 100%|██████████| 1716/1716 [00:55<00:00, 30.88batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13, Iter: 1715, Train Loss: 0.1075, Val Loss: 0.4779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/15:  72%|███████▏  | 1227/1716 [00:39<00:15, 30.71batch/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-397e9c2322f7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Standard forward pass (no mixed precision)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m                 for hook_id, hook in (\n",
            "\u001b[0;32m<ipython-input-99-609447144f73>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mdotw_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdotw_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dotw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Day of the week embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoty_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoty_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doty'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Day of the year embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mcat_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Category embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Content-based filtering features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, offsets, per_sample_weights)\u001b[0m\n\u001b[1;32m    389\u001b[0m               \u001b[0mreturned\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0mfilled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \"\"\"\n\u001b[0;32m--> 391\u001b[0;31m         return F.embedding_bag(input, self.weight, offsets,\n\u001b[0m\u001b[1;32m    392\u001b[0m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding_bag\u001b[0;34m(input, weight, offsets, max_norm, norm_type, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx)\u001b[0m\n\u001b[1;32m   2452\u001b[0m         )\n\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m     ret, _, _, _ = torch.embedding_bag(\n\u001b[0m\u001b[1;32m   2455\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_enum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_last_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for num_iter, (features, target) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")):\n",
        "        features = to_device(features, device)\n",
        "        target = to_device(target, device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass (no mixed precision)!!!\n",
        "        pred = model(**features)\n",
        "        loss = criterion(pred, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        wandb.log({\n",
        "            'epoch': epoch,\n",
        "            'iteration': num_iter,\n",
        "            'train_loss': loss.item(),\n",
        "        })\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for val_features, val_target in val_loader:\n",
        "            val_features = to_device(val_features, device)\n",
        "            val_target = to_device(val_target, device)\n",
        "\n",
        "            # Standard forward pass (no mixed precision)\n",
        "            val_pred = model(**val_features)\n",
        "            val_loss += criterion(val_pred, val_target).item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "\n",
        "    scheduler.step(val_loss) \n",
        "    wandb.log({'val_loss': val_loss})\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Iter: {num_iter}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Document total training time and save model\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "\n",
        "wandb.log({'training_duration': duration})\n",
        "\n",
        "# Save the model\n",
        "model_path = \"model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Log the model artifact to W&B\n",
        "artifact = wandb.Artifact('model', type='model')\n",
        "artifact.add_file(model_path)\n",
        "wandb.log_artifact(artifact)\n",
        "\n",
        "# Finish W&B run\n",
        "wandb.finish()\n",
        "\n",
        "# Print the total training time\n",
        "print(f\"Total training time: {duration / 60:.2f} minutes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKcGOu7VXPpp"
      },
      "outputs": [],
      "source": [
        "test = train_dataset[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqQIwJ8K2G0H"
      },
      "outputs": [],
      "source": [
        "tokens = test[0]['tokens']\n",
        "mask = test[0]['att_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-dRRyAI4DYa"
      },
      "outputs": [],
      "source": [
        "tokens = torch.load('/content/drive/MyDrive/Machine Learning/BRS/input_ids.pt')\n",
        "att = torch.load('/content/drive/MyDrive/Machine Learning/BRS/mask.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jODECl84Esgw",
        "outputId": "8c20f180-acc5-4048-f099-e5e8e657f930"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 66993/66993 [3:34:42<00:00,  5.20it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import DistilBertModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DistilBertEmb(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        # Freeze parameters, do not want to fine-tune DistilBERT...\n",
        "        for param in self.distilbert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, tokens, att_mask):\n",
        "        # Forward pass\n",
        "        outputs = self.distilbert_model(tokens, attention_mask=att_mask)\n",
        "        # Get embeddings\n",
        "        emb = outputs.last_hidden_state\n",
        "        # Mean Pooling\n",
        "        pooled_tokens_emb = emb.mean(dim=1)  # Across sequence dimension\n",
        "        return pooled_tokens_emb\n",
        "\n",
        "\n",
        "def compute_embeddings(token_tensor, att_mask_tensor, distilbert_model, batch_size=64):  # Reduced batch size\n",
        "\n",
        "    distilbert_model.eval()\n",
        "\n",
        "    # Store embeddings\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Move tensors to gpu\n",
        "    token_tensor = token_tensor.to('cuda')\n",
        "    att_mask_tensor = att_mask_tensor.to('cuda')\n",
        "\n",
        "    # Loop + progress bar\n",
        "    for i in tqdm(range(0, token_tensor.size(0), batch_size)):\n",
        "        # Clear CUDA cache to prevent fragmentation issue\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Get the current batch\n",
        "        batch_tokens = token_tensor[i:i + batch_size]\n",
        "        batch_att_mask = att_mask_tensor[i:i + batch_size]\n",
        "\n",
        "        # Compute embeddings using mixed precision\n",
        "        # Cut foward pass runtime in half\n",
        "        with torch.no_grad():  # Disable gradients for faster computation\n",
        "            with torch.cuda.amp.autocast():  \n",
        "                embeddings = distilbert_model(batch_tokens, batch_att_mask)\n",
        "\n",
        "        # Append the embeddings to the lis\n",
        "        all_embeddings.append(embeddings.cpu())  \n",
        "\n",
        "    # Concatenate and return result \n",
        "    embeddings_tensor = torch.cat(all_embeddings, dim=0)\n",
        "\n",
        "    return embeddings_tensor\n",
        "\n",
        "\n",
        "distilbert_model = DistilBertEmb().to('cuda')  # Move to GPU if available\n",
        "embeddings_tensor = compute_embeddings(tokens, att, distilbert_model)\n",
        "torch.save(embeddings_tensor, '/content/drive/MyDrive/Machine Learning/BRS/embeddings_tensor.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpWCl2qEZYfT",
        "outputId": "fa2232f0-7e7b-4b79-b5c5-df12aa2ce4dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.1058,  0.0931,  0.2348,  ..., -0.0107,  0.0529, -0.1374],\n",
              "        [-0.0279,  0.0524,  0.2643,  ..., -0.0398,  0.1024, -0.0317],\n",
              "        [-0.0845,  0.0803,  0.2730,  ...,  0.0253,  0.1690, -0.0934],\n",
              "        ...,\n",
              "        [-0.0053,  0.2766,  0.1076,  ..., -0.0313,  0.1434, -0.1631],\n",
              "        [-0.2171,  0.1897,  0.2255,  ..., -0.1316,  0.2117, -0.1333],\n",
              "        [ 0.0913,  0.1682,  0.2498,  ...,  0.0447,  0.1635,  0.0219]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N9aeCz8bf24"
      },
      "outputs": [],
      "source": [
        "# torch.save(embeddings_tensor, '/content/drive/MyDrive/Machine Learning/BRS/embeddings_tensor2.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2GFttb5bi1L",
        "outputId": "e25f52f7-71dc-4c7d-ebb2-cf31cc334fd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_tensor.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26jzkHcIb-SQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00ff49daed4b4512b7a910bb36f8ecfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45c25f995dca46289974d1f9615da168": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fba069b5d9694969b0cc640a5be0899d",
              "IPY_MODEL_bf5027ff25644f3785924adc5262f770"
            ],
            "layout": "IPY_MODEL_f8e67d727d9a4435b67ff0b168ef1cf9"
          }
        },
        "b0c51219efcb4f7992f2f9757740c24e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf5027ff25644f3785924adc5262f770": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00ff49daed4b4512b7a910bb36f8ecfc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e476b4dd821844cd9422ab7fb42bc603",
            "value": 1
          }
        },
        "c658ab4fcc6c4666ae4b561c7a96bc80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e476b4dd821844cd9422ab7fb42bc603": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8e67d727d9a4435b67ff0b168ef1cf9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fba069b5d9694969b0cc640a5be0899d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0c51219efcb4f7992f2f9757740c24e",
            "placeholder": "​",
            "style": "IPY_MODEL_c658ab4fcc6c4666ae4b561c7a96bc80",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
