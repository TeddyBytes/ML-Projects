{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Data processing and numerical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import dask.dataframe as dd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and recommendation libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml import Pipeline\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "\n",
    "# IPython for displaying outputs\n",
    "from IPython.display import display\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split data once - no temporal features\n",
    "# train, temp = spark_df.randomSplit([0.75, 0.25], seed=42)\n",
    "# val, test = temp.randomSplit([.15, .10], seed=42)\n",
    "# # \n",
    "\n",
    "# # define file paths (relative to the current directory)\n",
    "# model_data_path = \"../../data/interim/\"\n",
    "\n",
    "# # save each DataFrame in parquet format\n",
    "# train.write.parquet(os.path.join(model_data_path, f\"train_set.parquet\"), mode='overwrite')\n",
    "# val.write.parquet(os.path.join(model_data_path, f\"val_set.parquet\"), mode='overwrite')\n",
    "# test.write.parquet(os.path.join(model_data_path, f\"test_set.parquet\"), mode='overwrite')\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# ========================================\n",
    "# Open sessions for necessary packages\n",
    "# ========================================\n",
    "# spark = None\n",
    "\n",
    "# def open_session(close=False):\n",
    "#     global spark  # \n",
    "#     if not close:\n",
    "#         if spark is None or spark.sparkContext is None:\n",
    "#             spark = SparkSession.builder \\\n",
    "#                 .appName(\"ALS in Spark\") \\\n",
    "#                 .getOrCreate()\n",
    "#             # set up MLflow (only needs to be done once)\n",
    "#     else:\n",
    "#         if spark is not None:\n",
    "#             spark.stop()\n",
    "#             spark = None\n",
    "            \n",
    "# # open_session()\n",
    "# open_session(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "# model_data_path = \"../data/interim/\"\n",
    "\n",
    "# train = spark.read.parquet(os.path.join(model_data_path, \"train_set.parquet\")).toPandas()\n",
    "# val = spark.read.parquet(os.path.join(model_data_path, \"val_set.parquet\")).toPandas()\n",
    "# test = spark.read.parquet(os.path.join(model_data_path, \"test_set.parquet\")).toPandas()\n",
    "\n",
    "# train.repartition(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define PySpark ALS model\n",
    "# als = ALS(\n",
    "#     userCol=\"user_index\",\n",
    "#     itemCol=\"bus_index\",\n",
    "#     ratingCol=\"rating\",\n",
    "#     coldStartStrategy=\"drop\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # # Grid search through hyperparameters\n",
    "# # paramGrid = (ParamGridBuilder()\n",
    "# #              .addGrid(als.rank, [5, 10, 15])\n",
    "# #              .addGrid(als.maxIter, [5, 10, 20])\n",
    "# #              .addGrid(als.regParam, [0.01, 0.1, 0.5])\n",
    "# #              .build())\n",
    "\n",
    "# # define criterion\n",
    "# evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "# # define cross-validation w simple grid\n",
    "# crossval = CrossValidator(\n",
    "#     estimator=als,\n",
    "#     evaluator=evaluator,\n",
    "#     estimatorParamMaps=paramGrid,\n",
    "#     numFolds=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = train.rdd\n",
    "# partitions = rdd.glom().collect()\n",
    "# for index, partition in enumerate(partitions):\n",
    "#     print(f\"Partition {index} contains {len(partition)} rows.\")\n",
    "#     if len(partition) > 0:\n",
    "#         print(f\"Sample data from partition {index}: {partition[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# open log \n",
    "# mlflow.set_experiment(\"ALS_Hyperparameter_Tuning\")\n",
    "\n",
    "# log results\n",
    "# with mlflow.start_run():\n",
    "    \n",
    "    # fit model using cross-validation\n",
    "    # cv_model = crossval.fit(train)\n",
    "    \n",
    "    # Log the best model\n",
    "    # best_model = cv_model.bestModel\n",
    "    # mlflow.spark.log_model(best_model, \"best_model\")\n",
    "    \n",
    "    # # Log metrics and model parameters for each parameter combination\n",
    "    # for param_map, metric in zip(crossval.getEstimatorParamMaps(), cv_model.avgMetrics):\n",
    "    #     rank = param_map[als.rank]\n",
    "    #     regParam = param_map[als.regParam]\n",
    "    #     maxIter = param_map[als.maxIter]\n",
    "        \n",
    "    #     mlflow.log_param(\"rank\", rank)\n",
    "    #     mlflow.log_param(\"regParam\", regParam)\n",
    "    #     mlflow.log_param(\"maxIter\", maxIter)\n",
    "    #     mlflow.log_metric(\"validation_rmse\", metric)\n",
    "\n",
    "    # # Log validation scores\n",
    "    # validation_predictions = best_model.transform(val)\n",
    "    # validation_rmse = evaluator.evaluate(validation_predictions)\n",
    "    # mlflow.log_metric(\"validation_rmse\", validation_rmse)\n",
    "\n",
    "\n",
    "# # Log test metrics (optional, after final model selection)\n",
    "# test_predictions = best_model.transform(test)\n",
    "# test_rmse = evaluator.evaluate(test_predictions)\n",
    "# mlflow.log_metric(\"test_rmse\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define paths\n",
    "# model_data_path = \"../data/interim/\"\n",
    "\n",
    "# # read data and conver to pandas df\n",
    "# train = spark.read.parquet(os.path.join(model_data_path, \"train_set.parquet\")).toPandas()\n",
    "# val = spark.read.parquet(os.path.join(model_data_path, \"val_set.parquet\")).toPandas()\n",
    "# test = spark.read.parquet(os.path.join(model_data_path, \"test_set.parquet\")).toPandas()\n",
    "\n",
    "# def convert_to_tensors(df):\n",
    "#     ratings_tensor = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "#     user_id_tensor = torch.tensor(df['user_index'].values, dtype=torch.int64)\n",
    "#     bus_id_tensor = torch.tensor(df['bus_index'].values, dtype=torch.int64)\n",
    "#     return ratings_tensor, user_id_tensor, bus_id_tensor\n",
    "\n",
    "# def save_tensors(prefix, **tensors):\n",
    "#     for name, tensor in tensors.items():\n",
    "#         file_path = os.path.join(model_data_path, f'{prefix}_{name}.pt')\n",
    "#         torch.save(tensor, file_path)\n",
    "\n",
    "# # Convert data to tensors\n",
    "# train_tensors = convert_to_tensors(train)\n",
    "# val_tensors = convert_to_tensors(val)\n",
    "# test_tensors = convert_to_tensors(test)\n",
    "\n",
    "# # Save tensors\n",
    "# save_tensors('train', ratings=train_tensors[0], user_id=train_tensors[1], bus_id=train_tensors[2])\n",
    "# save_tensors('val', ratings=val_tensors[0], user_id=val_tensors[1], bus_id=val_tensors[2])\n",
    "# save_tensors('test', ratings=test_tensors[0], user_id=test_tensors[1], bus_id=test_tensors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_data_path = \"../data/interim/\"\n",
    "\n",
    "data_path = mac_data_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "mac_data_path = \"../data/processed/Final Dataframes\"\n",
    "\n",
    "data_path = mac_data_path\n",
    "\n",
    "# # Check if Mac GPU is available\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Check if nvidia gpu is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# df = pd.read_parquet(os.path.join(mac_data_path, f\"final_df.parquet\"))\n",
    "\n",
    "# train, val_test = train_test_split(df, test_size=.20, shuffle=True)\n",
    "# val, test = train_test_split(val_test, test_size=.5, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.features = {\n",
    "            'user_id': torch.tensor(dataframe['user_num_id'].values, dtype=torch.long),\n",
    "            'business_id': torch.tensor(dataframe['bus_num_id'].values, dtype=torch.long),\n",
    "            'city_id': torch.tensor(dataframe['city_code'].values, dtype=torch.long),\n",
    "            'state_id': torch.tensor(dataframe['state_code'].values, dtype=torch.long),\n",
    "            'region_id': torch.tensor(dataframe['region_code'].values, dtype=torch.long),\n",
    "            'dotw': torch.tensor(dataframe['day_of_week'].values, dtype=torch.long),\n",
    "            'doty': torch.tensor(dataframe['day_of_year'].values, dtype=torch.long),\n",
    "            'numerical_features': torch.tensor(dataframe[['user_avg_rating_norm', \n",
    "                                                          'bus_avg_rating_norm', \n",
    "                                                          'log_business_review_count_norm', \n",
    "                                                          'log_user_review_count_norm', \n",
    "                                                          'years_yelp_member_norm', \n",
    "                                                          'years_since_review_norm']]\n",
    "                                               .values,\n",
    "                                               dtype=torch.float),\n",
    "            'tokens': torch.tensor(dataframe['tokens'].tolist(), dtype=torch.long)\n",
    "        }\n",
    "        self.target = torch.tensor(dataframe['mean_centered_rating'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.features.items()}, self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 287116\n",
      "Number of unique business's : 148523\n"
     ]
    }
   ],
   "source": [
    "# Concat user and business data into one tensor\n",
    "total_user_ids = torch.concat((train.users, val.users, test.users), dim=0)\n",
    "total_business_ids = torch.concat((train.businesses, val.businesses, test.businesses), dim=0)\n",
    "\n",
    "# get number of unique entities\n",
    "num_unique_users = torch.unique(total_user_ids).shape[0] \n",
    "num_unique_bus = torch.unique(total_business_ids).shape[0]\n",
    "\n",
    "# output findings\n",
    "print(f\"Number of unique users: {num_unique_users}\")\n",
    "print(f\"Number of unique business's : {num_unique_bus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if nvidia gpu is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFmodel(nn.Module):\n",
    "    def __init__(self, rank=9, num_users=287116, num_bus=148523, num_city=1273, num_regions=11, num_states=50, token_length=10):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.user_emb = nn.Embedding(num_users, rank)\n",
    "        self.bus_emb = nn.Embedding(num_bus, rank)\n",
    "        self.city_emb = nn.Embedding(num_city, rank//3)\n",
    "        self.state_emb = nn.Embedding(num_states, rank//3)\n",
    "        self.region_emb = nn.Embedding(num_regions, rank//3)\n",
    "        self.dotw = nn.Embedding(7, rank)\n",
    "        self.doty = nn.Embedding(367, rank)\n",
    "        self.fc1 = nn.Linear(rank, 1)\n",
    "        self.token_len = token_length\n",
    "        \n",
    "        # add a layer for numerical features\n",
    "        self.numerical_layer = nn.Linear(6, rank)  # 6 is the number of numerical features\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        user_emb = self.user_emb(kwargs['user_id'])     \n",
    "        bus_emb = self.bus_emb(kwargs['business_id']) \n",
    "        \n",
    "        city_emb = self.city_emb(kwargs['city_id'])\n",
    "        state_emb = self.state_emb(kwargs['state_id'])\n",
    "        region_emb = self.region_emb(kwargs['region_id'])\n",
    "        \n",
    "        location = torch.cat((city_emb, state_emb, region_emb), dim=1)\n",
    "        \n",
    "        dotw = self.dotw(kwargs['dotw'])\n",
    "        doty = self.doty(kwargs['doty'])\n",
    "        \n",
    "        numerical = self.numerical_layer(kwargs['numerical_features'])\n",
    "        \n",
    "        product = user_emb * bus_emb\n",
    "        \n",
    "        prod_loc = product + location + dotw + doty + numerical\n",
    "        result = self.fc1(prod_loc)\n",
    "        \n",
    "        return result.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First iteration of the model, I will be just using cosine similiarity and storing results in mlflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "rank = 10\n",
    "batch_size = 64 \n",
    "num_epochs = 1\n",
    "\n",
    "train_dataset = CFDataset(train)\n",
    "val_dataset = CFDataset(val)\n",
    "test_dataset = CFDataset(test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and define optimization\n",
    "\n",
    "model = CFmodel()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),    \n",
    "    lr=0.001,              \n",
    "    betas=(0.9, 0.999),    \n",
    "    eps=1e-08,             \n",
    "    weight_decay=.01 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wandb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[219], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Initialize W&B\u001b[39;00m\n\u001b[1;32m      3\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitial Model\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace with your project and username\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project='Initial Model')  # Replace with your project and username\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0, loss: 2.1258974075317383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m_zmq.py:160\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython._zmq.Frame.__del__'\n",
      "Traceback (most recent call last):\n",
      "  File \"_zmq.py\", line 160, in zmq.backend.cython._zmq._check_rc\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  <built-in function iter>, Train Loss: 2.1259, Val Loss: 2.5342\n",
      "Iterations: 100, loss: 1.5003511905670166\n",
      "Iter:  <built-in function iter>, Train Loss: 1.5004, Val Loss: 1.8582\n",
      "Iterations: 200, loss: 1.8833609819412231\n",
      "Iter:  <built-in function iter>, Train Loss: 1.8834, Val Loss: 1.6077\n",
      "Iterations: 300, loss: 1.9653592109680176\n",
      "Iter:  <built-in function iter>, Train Loss: 1.9654, Val Loss: 1.5065\n",
      "Iterations: 400, loss: 1.2224197387695312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[218], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     30\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Machine Learning Projects/Business Recommendation System/brs/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/Machine Learning Projects/Business Recommendation System/brs/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/Machine Learning Projects/Business Recommendation System/brs/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, dict):\n",
    "        return {k: v.to(device) for k, v in data.items()}\n",
    "    return data.to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CFmodel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    num_iter = 0\n",
    "    for features, target in train_loader:\n",
    "        features = to_device(features, device)\n",
    "        target = to_device(target, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(**features)\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if num_iter % 1000 == 0:\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for features, target in val_loader:\n",
    "                    features = to_device(features, device)\n",
    "                    target = to_device(target, device)\n",
    "                    \n",
    "                    pred = model(**features)\n",
    "                    val_loss += criterion(pred, target).item()\n",
    "                \n",
    "                val_loss /= len(val_loader)\n",
    "            \n",
    "            # Log metrics to W&B\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'iteration': num_iter,\n",
    "                'train_loss': loss.item(),\n",
    "                'val_loss': val_loss\n",
    "            })\n",
    "            \n",
    "            print(f\"Epoch: {epoch}, Iter: {num_iter}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        num_iter += 1\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
