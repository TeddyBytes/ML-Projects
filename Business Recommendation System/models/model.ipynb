{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Data processing and numerical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import dask.dataframe as dd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and recommendation libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml import Pipeline\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "\n",
    "# IPython for displaying outputs\n",
    "from IPython.display import display\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split data once - no temporal features\n",
    "# train, temp = spark_df.randomSplit([0.75, 0.25], seed=42)\n",
    "# val, test = temp.randomSplit([.15, .10], seed=42)\n",
    "# # \n",
    "\n",
    "# # define file paths (relative to the current directory)\n",
    "# model_data_path = \"../../data/interim/\"\n",
    "\n",
    "# # save each DataFrame in parquet format\n",
    "# train.write.parquet(os.path.join(model_data_path, f\"train_set.parquet\"), mode='overwrite')\n",
    "# val.write.parquet(os.path.join(model_data_path, f\"val_set.parquet\"), mode='overwrite')\n",
    "# test.write.parquet(os.path.join(model_data_path, f\"test_set.parquet\"), mode='overwrite')\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# ========================================\n",
    "# Open sessions for necessary packages\n",
    "# ========================================\n",
    "# spark = None\n",
    "\n",
    "# def open_session(close=False):\n",
    "#     global spark  # \n",
    "#     if not close:\n",
    "#         if spark is None or spark.sparkContext is None:\n",
    "#             spark = SparkSession.builder \\\n",
    "#                 .appName(\"ALS in Spark\") \\\n",
    "#                 .getOrCreate()\n",
    "#             # set up MLflow (only needs to be done once)\n",
    "#     else:\n",
    "#         if spark is not None:\n",
    "#             spark.stop()\n",
    "#             spark = None\n",
    "            \n",
    "# # open_session()\n",
    "# open_session(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "# model_data_path = \"../data/interim/\"\n",
    "\n",
    "# train = spark.read.parquet(os.path.join(model_data_path, \"train_set.parquet\")).toPandas()\n",
    "# val = spark.read.parquet(os.path.join(model_data_path, \"val_set.parquet\")).toPandas()\n",
    "# test = spark.read.parquet(os.path.join(model_data_path, \"test_set.parquet\")).toPandas()\n",
    "\n",
    "# train.repartition(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define PySpark ALS model\n",
    "# als = ALS(\n",
    "#     userCol=\"user_index\",\n",
    "#     itemCol=\"bus_index\",\n",
    "#     ratingCol=\"rating\",\n",
    "#     coldStartStrategy=\"drop\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # # Grid search through hyperparameters\n",
    "# # paramGrid = (ParamGridBuilder()\n",
    "# #              .addGrid(als.rank, [5, 10, 15])\n",
    "# #              .addGrid(als.maxIter, [5, 10, 20])\n",
    "# #              .addGrid(als.regParam, [0.01, 0.1, 0.5])\n",
    "# #              .build())\n",
    "\n",
    "# # define criterion\n",
    "# evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "# # define cross-validation w simple grid\n",
    "# crossval = CrossValidator(\n",
    "#     estimator=als,\n",
    "#     evaluator=evaluator,\n",
    "#     estimatorParamMaps=paramGrid,\n",
    "#     numFolds=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = train.rdd\n",
    "# partitions = rdd.glom().collect()\n",
    "# for index, partition in enumerate(partitions):\n",
    "#     print(f\"Partition {index} contains {len(partition)} rows.\")\n",
    "#     if len(partition) > 0:\n",
    "#         print(f\"Sample data from partition {index}: {partition[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# open log \n",
    "# mlflow.set_experiment(\"ALS_Hyperparameter_Tuning\")\n",
    "\n",
    "# log results\n",
    "# with mlflow.start_run():\n",
    "    \n",
    "    # fit model using cross-validation\n",
    "    # cv_model = crossval.fit(train)\n",
    "    \n",
    "    # Log the best model\n",
    "    # best_model = cv_model.bestModel\n",
    "    # mlflow.spark.log_model(best_model, \"best_model\")\n",
    "    \n",
    "    # # Log metrics and model parameters for each parameter combination\n",
    "    # for param_map, metric in zip(crossval.getEstimatorParamMaps(), cv_model.avgMetrics):\n",
    "    #     rank = param_map[als.rank]\n",
    "    #     regParam = param_map[als.regParam]\n",
    "    #     maxIter = param_map[als.maxIter]\n",
    "        \n",
    "    #     mlflow.log_param(\"rank\", rank)\n",
    "    #     mlflow.log_param(\"regParam\", regParam)\n",
    "    #     mlflow.log_param(\"maxIter\", maxIter)\n",
    "    #     mlflow.log_metric(\"validation_rmse\", metric)\n",
    "\n",
    "    # # Log validation scores\n",
    "    # validation_predictions = best_model.transform(val)\n",
    "    # validation_rmse = evaluator.evaluate(validation_predictions)\n",
    "    # mlflow.log_metric(\"validation_rmse\", validation_rmse)\n",
    "\n",
    "\n",
    "# # Log test metrics (optional, after final model selection)\n",
    "# test_predictions = best_model.transform(test)\n",
    "# test_rmse = evaluator.evaluate(test_predictions)\n",
    "# mlflow.log_metric(\"test_rmse\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define paths\n",
    "# model_data_path = \"../data/interim/\"\n",
    "\n",
    "# # read data and conver to pandas df\n",
    "# train = spark.read.parquet(os.path.join(model_data_path, \"train_set.parquet\")).toPandas()\n",
    "# val = spark.read.parquet(os.path.join(model_data_path, \"val_set.parquet\")).toPandas()\n",
    "# test = spark.read.parquet(os.path.join(model_data_path, \"test_set.parquet\")).toPandas()\n",
    "\n",
    "# def convert_to_tensors(df):\n",
    "#     ratings_tensor = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "#     user_id_tensor = torch.tensor(df['user_index'].values, dtype=torch.int64)\n",
    "#     bus_id_tensor = torch.tensor(df['bus_index'].values, dtype=torch.int64)\n",
    "#     return ratings_tensor, user_id_tensor, bus_id_tensor\n",
    "\n",
    "# def save_tensors(prefix, **tensors):\n",
    "#     for name, tensor in tensors.items():\n",
    "#         file_path = os.path.join(model_data_path, f'{prefix}_{name}.pt')\n",
    "#         torch.save(tensor, file_path)\n",
    "\n",
    "# # Convert data to tensors\n",
    "# train_tensors = convert_to_tensors(train)\n",
    "# val_tensors = convert_to_tensors(val)\n",
    "# test_tensors = convert_to_tensors(test)\n",
    "\n",
    "# # Save tensors\n",
    "# save_tensors('train', ratings=train_tensors[0], user_id=train_tensors[1], bus_id=train_tensors[2])\n",
    "# save_tensors('val', ratings=val_tensors[0], user_id=val_tensors[1], bus_id=val_tensors[2])\n",
    "# save_tensors('test', ratings=test_tensors[0], user_id=test_tensors[1], bus_id=test_tensors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_data_path = \"../data/interim/\"\n",
    "\n",
    "data_path = mac_data_path\n",
    "\n",
    "pd.read_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "mac_data_path = \"../data/processed/Final Dataframes\"\n",
    "\n",
    "data_path = mac_data_path\n",
    "\n",
    "# # Check if Mac GPU is available\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Check if nvidia gpu is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "df = pd.read_parquet(os.path.join(mac_data_path, f\"final_df.parquet\"))\n",
    "\n",
    "train, val_test = train_test_split(df, test_size=.20, shuffle=True)\n",
    "val, test = train_test_split(val_test, test_size=.5, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFDataset(Dataset):\n",
    "    def __init__(self, dataframe):    \n",
    "        # UI embeddings\n",
    "        self.users = torch.tensor(dataframe['user_num_id'].values, dtype=torch.long)\n",
    "        self.businesses = torch.tensor(dataframe['bus_num_id'].values, dtype=torch.long)\n",
    "        # dt embeddings\n",
    "        self.dotw = torch.tensor(dataframe['day_of_week'].values, dtype=torch.long)\n",
    "        self.doty = torch.tensor(dataframe['day_of_year'].values, dtype=torch.long)\n",
    "        # geography embeddings\n",
    "        self.region_code = torch.tensor(dataframe['region_code'].values, dtype=torch.long)\n",
    "        self.state_code = torch.tensor(dataframe['state_code'].values, dtype=torch.long)\n",
    "        self.city_code = torch.tensor(dataframe['city_code'].values, dtype=torch.long)\n",
    "        \n",
    "        # Numerical features\n",
    "        self.numerical_features = torch.tensor(dataframe[['user_avg_rating_norm', \n",
    "                                                          'bus_avg_rating_norm', \n",
    "                                                          'log_business_review_count_norm', \n",
    "                                                          'log_user_review_count_norm', \n",
    "                                                          'years_yelp_member_norm', \n",
    "                                                          'years_since_review_norm']]\n",
    "                                               .values,\n",
    "                                               dtype=torch.float)\n",
    "        # tokens\n",
    "        self.tokens = torch.tensor(dataframe['tokens'].tolist(), dtype=torch.long)\n",
    "        \n",
    "        # Target (ratings)\n",
    "        self.ratings = torch.tensor(dataframe['mean_centered_rating'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = {\n",
    "            'users': self.users[idx],\n",
    "            'businesses': self.businesses[idx],\n",
    "            'dotw': self.dotw[idx],\n",
    "            'doty': self.doty[idx],\n",
    "            'region_code': self.region_code[idx],\n",
    "            'state_code': self.state_code[idx],\n",
    "            'city_code': self.city_code[idx],\n",
    "            'numerical_features': self.numerical_features[idx]\n",
    "        }\n",
    "        \n",
    "        target = self.ratings[idx]\n",
    "        \n",
    "        return features, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Adjust as needed\n",
    "\n",
    "train_dataset = CFDataset(train)\n",
    "val_dataset = CFDataset(val)\n",
    "test_dataset = CFDataset(test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 287116\n",
      "Number of unique business's : 148523\n"
     ]
    }
   ],
   "source": [
    "# Concat user and business data into one tensor\n",
    "total_user_ids = torch.concat((train.users, val.users, test.users), dim=0)\n",
    "total_business_ids = torch.concat((train.businesses, val.businesses, test.businesses), dim=0)\n",
    "\n",
    "# get number of unique entities\n",
    "num_unique_users = torch.unique(total_user_ids).shape[0] \n",
    "num_unique_bus = torch.unique(total_business_ids).shape[0]\n",
    "\n",
    "# output findings\n",
    "print(f\"Number of unique users: {num_unique_users}\")\n",
    "print(f\"Number of unique business's : {num_unique_bus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "\n",
    "# initially starting with just cosine similiarity\n",
    "class CFmodel(nn.Module):\n",
    "    \n",
    "    def __init__ (self, rank, num_users, num_bus):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.user_emb = nn.Embedding(num_users, rank)\n",
    "        self.bus_emb = nn.Embedding(num_bus, rank)\n",
    "        self.fc1 = nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self, user_id, business_id):\n",
    "        # get entity ids\n",
    "        user_emb = self.user_emb(user_id)     \n",
    "        bus_emb = self.bus_emb(business_id) \n",
    "        \n",
    "        \n",
    "        # # calulcate similarities\n",
    "        product = user_emb * bus_emb # element wise / essentially dot product of sparse matrix \n",
    "        cos_sim = product.sum(dim=1, keepdim=True)\n",
    "        result = self.fc1(cos_sim)\n",
    "        \n",
    "        return result.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First iteration of the model, I will be just using cosine similiarity and storing results in mlflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "rank = 10\n",
    "batch_size = 1000\n",
    "num_epochs = 3000\n",
    "# # create data \n",
    "# users = torch.randint(num_users, (data_size,), requires_grad=False)\n",
    "# business = torch.randint(num_business, (data_size,), requires_grad=False)\n",
    "# ratings = torch.randint(low=1, high=5, size=(data_size,), requires_grad=False).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model and define optimization\n",
    "\n",
    "model = MatrixFact(rank, num_unique_users, num_unique_bus)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_data = CustomDataset(train_bus_id, train_user_id, train_ratings)\n",
    "train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),    # Parameters of the model to optimize\n",
    "    lr=0.001,              # Learning rate (default is 0.001)\n",
    "    betas=(0.9, 0.999),    # Coefficients for computing running averages of gradient and its square\n",
    "    eps=1e-08,             # Term added to the denominator to improve numerical stability\n",
    "    weight_decay=.01       # Weight decay (L2 penalty)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
