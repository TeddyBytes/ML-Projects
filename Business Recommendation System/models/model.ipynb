{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewaudley/Documents/Machine Learning Projects/Business Recommendation System/brs/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Data processing and numerical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import dask.dataframe as dd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Machine learning and recommendation libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml import Pipeline\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import wandb\n",
    "\n",
    "# IPython for displaying outputs\n",
    "from IPython.display import display\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split data once - no temporal features\n",
    "# train, temp = spark_df.randomSplit([0.75, 0.25], seed=42)\n",
    "# val, test = temp.randomSplit([.15, .10], seed=42)\n",
    "# # \n",
    "\n",
    "# # define file paths (relative to the current directory)\n",
    "# model_data_path = \"../../data/interim/\"\n",
    "\n",
    "# # save each DataFrame in parquet format\n",
    "# train.write.parquet(os.path.join(model_data_path, f\"train_set.parquet\"), mode='overwrite')\n",
    "# val.write.parquet(os.path.join(model_data_path, f\"val_set.parquet\"), mode='overwrite')\n",
    "# test.write.parquet(os.path.join(model_data_path, f\"test_set.parquet\"), mode='overwrite')\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# ========================================\n",
    "# Open sessions for necessary packages\n",
    "# ========================================\n",
    "# spark = None\n",
    "\n",
    "# def open_session(close=False):\n",
    "#     global spark  # \n",
    "#     if not close:\n",
    "#         if spark is None or spark.sparkContext is None:\n",
    "#             spark = SparkSession.builder \\\n",
    "#                 .appName(\"ALS in Spark\") \\\n",
    "#                 .getOrCreate()\n",
    "#             # set up MLflow (only needs to be done once)\n",
    "#     else:\n",
    "#         if spark is not None:\n",
    "#             spark.stop()\n",
    "#             spark = None\n",
    "            \n",
    "# # open_session()\n",
    "# open_session(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "# model_data_path = \"../data/interim/\"\n",
    "\n",
    "# train = spark.read.parquet(os.path.join(model_data_path, \"train_set.parquet\")).toPandas()\n",
    "# val = spark.read.parquet(os.path.join(model_data_path, \"val_set.parquet\")).toPandas()\n",
    "# test = spark.read.parquet(os.path.join(model_data_path, \"test_set.parquet\")).toPandas()\n",
    "\n",
    "# train.repartition(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define PySpark ALS model\n",
    "# als = ALS(\n",
    "#     userCol=\"user_index\",\n",
    "#     itemCol=\"bus_index\",\n",
    "#     ratingCol=\"rating\",\n",
    "#     coldStartStrategy=\"drop\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # # Grid search through hyperparameters\n",
    "# # paramGrid = (ParamGridBuilder()\n",
    "# #              .addGrid(als.rank, [5, 10, 15])\n",
    "# #              .addGrid(als.maxIter, [5, 10, 20])\n",
    "# #              .addGrid(als.regParam, [0.01, 0.1, 0.5])\n",
    "# #              .build())\n",
    "\n",
    "# # define criterion\n",
    "# evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "# # define cross-validation w simple grid\n",
    "# crossval = CrossValidator(\n",
    "#     estimator=als,\n",
    "#     evaluator=evaluator,\n",
    "#     estimatorParamMaps=paramGrid,\n",
    "#     numFolds=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = train.rdd\n",
    "# partitions = rdd.glom().collect()\n",
    "# for index, partition in enumerate(partitions):\n",
    "#     print(f\"Partition {index} contains {len(partition)} rows.\")\n",
    "#     if len(partition) > 0:\n",
    "#         print(f\"Sample data from partition {index}: {partition[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# open log \n",
    "# mlflow.set_experiment(\"ALS_Hyperparameter_Tuning\")\n",
    "\n",
    "# log results\n",
    "# with mlflow.start_run():\n",
    "    \n",
    "    # fit model using cross-validation\n",
    "    # cv_model = crossval.fit(train)\n",
    "    \n",
    "    # Log the best model\n",
    "    # best_model = cv_model.bestModel\n",
    "    # mlflow.spark.log_model(best_model, \"best_model\")\n",
    "    \n",
    "    # # Log metrics and model parameters for each parameter combination\n",
    "    # for param_map, metric in zip(crossval.getEstimatorParamMaps(), cv_model.avgMetrics):\n",
    "    #     rank = param_map[als.rank]\n",
    "    #     regParam = param_map[als.regParam]\n",
    "    #     maxIter = param_map[als.maxIter]\n",
    "        \n",
    "    #     mlflow.log_param(\"rank\", rank)\n",
    "    #     mlflow.log_param(\"regParam\", regParam)\n",
    "    #     mlflow.log_param(\"maxIter\", maxIter)\n",
    "    #     mlflow.log_metric(\"validation_rmse\", metric)\n",
    "\n",
    "    # # Log validation scores\n",
    "    # validation_predictions = best_model.transform(val)\n",
    "    # validation_rmse = evaluator.evaluate(validation_predictions)\n",
    "    # mlflow.log_metric(\"validation_rmse\", validation_rmse)\n",
    "\n",
    "\n",
    "# # Log test metrics (optional, after final model selection)\n",
    "# test_predictions = best_model.transform(test)\n",
    "# test_rmse = evaluator.evaluate(test_predictions)\n",
    "# mlflow.log_metric(\"test_rmse\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.features = {\n",
    "            'user_id': torch.tensor(dataframe['user_num_id'].values, dtype=torch.long),\n",
    "            'business_id': torch.tensor(dataframe['business_num_id'].values, dtype=torch.long),\n",
    "            # 'city_id': torch.tensor(dataframe['city_code'].values, dtype=torch.long),\n",
    "            # 'state_id': torch.tensor(dataframe['state_code'].values, dtype=torch.long),\n",
    "            # 'region_id': torch.tensor(dataframe['region_code'].values, dtype=torch.long),\n",
    "            # 'dotw': torch.tensor(dataframe['day_of_week'].values, dtype=torch.long),\n",
    "            # 'doty': torch.tensor(dataframe['day_of_year'].values, dtype=torch.long),\n",
    "            # 'numerical_features': torch.tensor(dataframe[['user_avg_rating_norm', \n",
    "            #                                               'bus_avg_rating_norm', \n",
    "            #                                               'log_business_review_count_norm', \n",
    "            #                                               'log_user_review_count_norm', \n",
    "            #                                               'years_yelp_member_norm', \n",
    "            #                                               'years_since_review_norm']]\n",
    "            #                                    .values,\n",
    "            #                                    dtype=torch.float),\n",
    "            # 'pca_features': torch.tensor(dataframe[['pca_1', 'pca_2', 'pca_3', \n",
    "            #                                         'pca_4', 'pca_5']].values, dtype=torch.float32),\n",
    "            # 'tokens': torch.tensor(dataframe['tokens'].tolist(), dtype=torch.long),\n",
    "            # 'categories' : torch.tensor(dataframe['categories_enc'].tolist(), dtype=torch.long)\n",
    "        }\n",
    "        self.target = torch.tensor(dataframe['mean_centered_rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.features.items()}, self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets have been split and saved.\n",
      "Data loaded as DataFrame.\n"
     ]
    }
   ],
   "source": [
    "def split_and_save_data(dataframe, test_size=0.2, random_state=42):\n",
    "    # split data\n",
    "    train_df, temp_df = train_test_split(dataframe, test_size=test_size, random_state=random_state)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=.5, random_state=random_state)\n",
    "\n",
    "    # create datasets\n",
    "    train_dataset = CFDataset(train_df)\n",
    "    val_dataset = CFDataset(val_df)\n",
    "    test_dataset = CFDataset(test_df)\n",
    "\n",
    "    # import and save dataset as tensor\n",
    "    torch.save(train_dataset, os.path.join(tensor_path, 'train_dataset.pt'))\n",
    "    torch.save(val_dataset, os.path.join(tensor_path, 'val_dataset.pt'))\n",
    "    torch.save(test_dataset, os.path.join(tensor_path, 'test_dataset.pt'))\n",
    "    \n",
    "    \n",
    "    # save dataset as df\n",
    "    train_df.to_parquet(os.path.join(df_path, 'train_df.parquet'))\n",
    "    val_df.to_parquet(os.path.join(df_path, 'val_df.parquet'))\n",
    "    test_df.to_parquet(os.path.join(df_path, 'test_df.parquet'))\n",
    "\n",
    "    print(\"Datasets have been split and saved.\")\n",
    "\n",
    "def load_datasets(as_Tensor=True):\n",
    "    \n",
    "    if as_Tensor:\n",
    "        train = torch.load(os.path.join(tensor_path, 'train_dataset.pt'))\n",
    "        val= torch.load(os.path.join(tensor_path, 'val_dataset.pt'))\n",
    "        test = torch.load(os.path.join(tensor_path, 'test_dataset.pt'))\n",
    "    else:\n",
    "        train = pd.read_parquet(os.path.join(df_path, 'train_df.parquet'))\n",
    "        val = pd.read_parquet(os.path.join(df_path, 'val_df.parquet'))\n",
    "        test = pd.read_parquet(os.path.join(df_path, 'test_df.parquet'))\n",
    "        \n",
    "    print(f\"Data loaded as {'tensor' if as_Tensor else 'DataFrame'}.\")\n",
    "\n",
    "        \n",
    "    return train, val, test\n",
    "\n",
    "final_df_path = \"../data/processed/Final Dataframes/final_df.parquet\"\n",
    "tensor_path = df_path = \"../data/processed/Final Tensors/\"\n",
    "temp_tensor_path = \"../data/processed/Temp Tensors/\"\n",
    "\n",
    "df = pd.read_parquet(final_df_path)\n",
    "\n",
    "split_and_save_data(df)\n",
    "# train_dataset, val_dataset, test_dataset = load_datasets(True)\n",
    "train_df, val_df, test_df = load_datasets(False)\n",
    "\n",
    "train_dataset = CFDataset(train_df)\n",
    "val_dataset = CFDataset(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': tensor([135044, 251524, 160163,  ...,  84981,  69299, 141271]),\n",
       " 'business_id': tensor([ 74868,  97998,  19175,  ...,  14868,  30678, 135453])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Concat user and business data into one tensor\u001b[39;00m\n\u001b[1;32m      2\u001b[0m total_user_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat((train_dataset\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m                                val_dataset\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m----> 4\u001b[0m                                \u001b[43mtest_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m total_business_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat((train_dataset\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m                                     val_dataset\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m                                     test_dataset\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m'\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# get number of unique entities\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Concat user and business data into one tensor\n",
    "total_user_ids = torch.concat((train_dataset.features['user_id'],\n",
    "                               val_dataset.features['user_id'],\n",
    "                               test_dataset.features['user_id']), dim=0)\n",
    "total_business_ids = torch.concat((train_dataset.features['business_id'],\n",
    "                                    val_dataset.features['business_id'],\n",
    "                                    test_dataset.features['business_id']), dim=0)\n",
    "\n",
    "# get number of unique entities\n",
    "num_unique_users = torch.unique(total_user_ids).shape[0] \n",
    "num_unique_bus = torch.unique(total_business_ids).shape[0]\n",
    "\n",
    "# output findings\n",
    "print(f\"Number of unique users: {num_unique_users}\")\n",
    "print(f\"Number of unique business's : {num_unique_bus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CFmodel(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, rank=32, num_users=287116, num_bus=148523, num_city=1273, num_pca=5,\n",
    "                 num_regions=11, num_states=50, token_length=10, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.token_length = token_length\n",
    "\n",
    "        # Embeddings\n",
    "        self.user_emb = nn.Embedding(num_users, rank)\n",
    "        self.bus_emb = nn.Embedding(num_bus, rank)\n",
    "        # self.city_emb = nn.Embedding(num_city, rank)\n",
    "        # self.state_emb = nn.Embedding(num_states, rank)\n",
    "        # self.region_emb = nn.Embedding(num_regions, rank)\n",
    "        # self.dotw_emb = nn.Embedding(7, rank)\n",
    "        # self.doty_emb = nn.Embedding(367, rank)\n",
    "        # self.token_emb = nn.Embedding(vocab_size, rank)\n",
    "\n",
    "\n",
    "\n",
    "        # # Layer for numerical features\n",
    "        # self.numerical_layer = nn.Sequential(\n",
    "        #     nn.Linear(5, rank),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.BatchNorm1d(rank)\n",
    "        # )\n",
    "\n",
    "        # # Multi-head attention for combining embeddings\n",
    "        # self.multihead_attn = nn.MultiheadAttention(embed_dim=rank, num_heads=num_heads)\n",
    "        \n",
    "        # # Final layers\n",
    "        # self.fc_layers = nn.Sequential(\n",
    "        #     nn.Linear(rank, 256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.BatchNorm1d(256),\n",
    "        #     nn.Linear(256, 128),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.BatchNorm1d(128),\n",
    "        #     nn.Linear(128, 64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.BatchNorm1d(64),\n",
    "        #     nn.Linear(64, 1)\n",
    "        # )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.rank*2, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout1d(.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        # Process embeddings\n",
    "        user_emb = self.user_emb(kwargs['user_id'])#.unsqueeze(0)\n",
    "        bus_emb = self.bus_emb(kwargs['business_id'])#.unsqueeze(0)\n",
    "        # city_emb = self.city_emb(kwargs['city_id']).unsqueeze(0)\n",
    "        # state_emb = self.state_emb(kwargs['state_id']).unsqueeze(0)\n",
    "        # region_emb = self.region_emb(kwargs['region_id']).unsqueeze(0)\n",
    "        # dotw_emb = self.dotw_emb(kwargs['dotw']).unsqueeze(0)\n",
    "        # doty_emb = self.doty_emb(kwargs['doty']).unsqueeze(0)\n",
    "        \n",
    "        # # Process numerical features\n",
    "        # numerical = self.numerical_layer(kwargs['numerical_features']).unsqueeze(0)\n",
    "        # Process PCA reduced Features\n",
    "        # pca_layer = self.numerical_layer(kwargs['pca_features'])\n",
    "\n",
    "        # # Process tokens (reviews)\n",
    "        # tokens_emb = self.token_emb(kwargs['tokens']).mean(dim=1).unsqueeze(0)  # Average pooling\n",
    "        \n",
    "        # # Combine all features using multi-head attention\n",
    "        # combined_features = torch.cat([\n",
    "        #     user_emb, bus_emb, city_emb, state_emb, region_emb, \n",
    "        #     dotw_emb, doty_emb, numerical, tokens_emb\n",
    "        # ], dim=0)\n",
    "\n",
    "        combined_features = torch.cat([user_emb, bus_emb], dim=1)\n",
    "        \n",
    "        # attn_output, _ = self.multihead_attn(combined_features, combined_features, combined_features)\n",
    "        \n",
    "        # Average the attention output\n",
    "        # final_representation = attn_output.mean(dim=0)\n",
    "        \n",
    "        # Pass through final layers\n",
    "        result = self.fc_layers(combined_features)\n",
    "        \n",
    "        return result.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int) -> None:\n",
    "        self.patience = patience\n",
    "        self.strikes = 0\n",
    "        self.best_model = None\n",
    "        self.prev_loss = float('inf')\n",
    "        self.min_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> bool:\n",
    "        # If the current validation loss is less than the previous best\n",
    "        if val_loss < self.prev_loss:\n",
    "            self.prev_loss = val_loss  # Update the previous loss\n",
    "            self.strikes = 0            # Reset strikes\n",
    "            if val_loss < self.min_loss:  # Check if it's the best model\n",
    "                self.best_model = model.state_dict()\n",
    "                self.min_loss = val_loss\n",
    "        else:  # If no improvement, increment strikes\n",
    "            self.strikes += 1\n",
    "            if self.strikes >= self.patience:  # Check for early stopping\n",
    "                \n",
    "                self.early_stop = True\n",
    "                \n",
    "                print(f\"Early Stopping incurred, after {self.patience} iteration(s) of increasing validation loss. \")\n",
    "                print(f\"Current validation loss: {val_loss}, best validation loss: {self.min_loss}\")\n",
    "\n",
    "        return self.early_stop\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        \"\"\"Return the best model's state dictionary.\"\"\"\n",
    "        return self.best_model\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        if self.strikes < self.patience:\n",
    "            return (f\"Current patience is {self.patience}. \\n\"\n",
    "                    f\"{self.strikes} strikes have occurred. \"\n",
    "                    f\"{self.patience - self.strikes} strikes left.\")\n",
    "        else:\n",
    "            return \"Early stopping has been triggered.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# # Hyperparameters\n",
    "rank = 10\n",
    "\n",
    "\n",
    "# Model / Optimizer Information \n",
    "batch_size = 80000\n",
    "lr = .002\n",
    "dataset=\"yelp\"\n",
    "num_epochs = 20\n",
    "betas = (0.9, 0.999)\n",
    "eps=1e-08\n",
    "weight_decay=.01 #L2 norm\n",
    "\n",
    "# # Check if Mac GPU is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_workers = 4 if device == \"cuda\" else 0\n",
    "pin_memory = True if device == \"cuda\" else False\n",
    "\n",
    "# Load datasets\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,         \n",
    "    pin_memory=pin_memory       \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,         \n",
    "    pin_memory=pin_memory       \n",
    ")\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     num_workers=8,       \n",
    "#     pin_memory=True      \n",
    "# )\n",
    "\n",
    "# # Check if Mac GPU is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Check if nvidia gpu is available\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize model and define optimization\n",
    "model = CFmodel(rank=10)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "early_stopper = EarlyStopping(patience=3)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),     # Parameters of the model to optimize\n",
    "    lr=lr,                 # Learning rate (default is 0.001)\n",
    "    betas=betas,           # Coefficients for computing running averages of gradient and its square\n",
    "    eps=eps,               # Term added to the denominator to improve numerical stability\n",
    "    weight_decay=weight_decay  # Weight decay (L2 penalty) applied directly\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CFmodel(\n",
       "  (user_emb): Embedding(287116, 10)\n",
       "  (bus_emb): Embedding(148523, 10)\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout1d(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad_norm(model):\n",
    "    # Calculate gradient norm\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)  # Compute L2 norm\n",
    "            total_norm += param_norm.item() ** 2\n",
    "\n",
    "    grad_norm = total_norm ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "def calulcate_val_loss(model):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_features, val_target in val_loader:\n",
    "            val_features = to_device(val_features, device)\n",
    "            val_target = to_device(val_target, device)\n",
    "\n",
    "            val_pred = model(**val_features)\n",
    "            val_loss += criterion(val_pred, val_target).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        return val_loss\n",
    "\n",
    "\n",
    "def log_performance(model, epoch=0, num_iter=0, train_loss=0, val_loss=None):\n",
    "                    \n",
    "    grad_norm = calculate_grad_norm(model=model)\n",
    "    \n",
    "    # Log performance to wandb\n",
    "    wandb.log({\n",
    "        'epoch': epoch,\n",
    "        'iteration': num_iter,\n",
    "        'loss_train': train_loss,\n",
    "        'gradient_norms' : grad_norm\n",
    "    })\n",
    "    \n",
    "    # log performance to tensorboard\n",
    "    writer.add_scalar('loss/train', train_loss, epoch * len(train_loader) + num_iter)\n",
    "    writer.add_scalar('gradients/norm', grad_norm, epoch * len(train_loader) + num_iter)\n",
    "    \n",
    "    # Log distribution of weights and grad\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(f'weights/{name}', param.data, epoch * len(train_loader) + num_iter)\n",
    "        writer.add_histogram(f'gradients/{name}', param.grad, epoch * len(train_loader) + num_iter)\n",
    "        \n",
    "        wandb.log({f'weights/{name}': param.data.cpu().numpy()})\n",
    "        wandb.log({f'gradients/{name}': param.grad.data.cpu().numpy()})\n",
    "          \n",
    "    if val_loss:\n",
    "        \n",
    "\n",
    "        val_loss = calulcate_val_loss(model)\n",
    "\n",
    "        # update logs \n",
    "        wandb.log({'loss/val': val_loss})\n",
    "        writer.add_scalar(\"loss/val\", val_loss)\n",
    "        \n",
    "        # Print current training stats\n",
    "        print(f\"Epoch: {epoch}, Iter: {num_iter}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "\n",
    "def log_closing_stats(model, start_time, save_dir=\"./Model history/\"):\n",
    "    # Calculate and log training duration\n",
    "    end_time = time.time()\n",
    "    train_duration = end_time - start_time\n",
    "    \n",
    "    # Log training duration to wandb and TensorBoard\n",
    "    wandb.log({'training_duration': train_duration})\n",
    "    writer.add_scalar(\"training_duration\", train_duration)\n",
    "    \n",
    "    # Print training duration\n",
    "    print(f\"Total training time: {train_duration / 60:.2f} minutes\")\n",
    "    \n",
    "    # Save the model with a timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") \n",
    "    model_path = f\"{save_dir}model_{timestamp}.pth\" # Create the model path\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path) # Save the model state\n",
    "    \n",
    "    # Log model to wandb as an artifact\n",
    "    artifact = wandb.Artifact('model', type='model')\n",
    "    artifact.add_file(model_path) # Attach model file\n",
    "    wandb.log_artifact(artifact) # Log artifact to wandb\n",
    "    \n",
    "\n",
    "    return train_duration\n",
    "\n",
    "# load data onto gpu\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, dict):\n",
    "        return {k: v.to(device) for k, v in data.items()}\n",
    "    return data.to(device)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:giw124oo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-music-4</strong> at: <a href='https://wandb.ai/teddybytesorg/New%20Model/runs/giw124oo' target=\"_blank\">https://wandb.ai/teddybytesorg/New%20Model/runs/giw124oo</a><br/> View project at: <a href='https://wandb.ai/teddybytesorg/New%20Model' target=\"_blank\">https://wandb.ai/teddybytesorg/New%20Model</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240920_203630-giw124oo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:giw124oo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/matthewaudley/Documents/Machine Learning Projects/Business Recommendation System/models/wandb/run-20240920_203810-uuuemyui</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/teddybytesorg/New%20Model/runs/uuuemyui' target=\"_blank\">efficient-fog-5</a></strong> to <a href='https://wandb.ai/teddybytesorg/New%20Model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/teddybytesorg/New%20Model' target=\"_blank\">https://wandb.ai/teddybytesorg/New%20Model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/teddybytesorg/New%20Model/runs/uuuemyui' target=\"_blank\">https://wandb.ai/teddybytesorg/New%20Model/runs/uuuemyui</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# watch -n 1 nvidia-smi\n",
    "\n",
    "# Initialize W&B to watch the model\n",
    "wandb.init(project=\"New Model\", config={\n",
    "    \"descr\": \" user emb only\",\n",
    "    \"learning_rate\": lr,\n",
    "    \"betas\": betas,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"rank\": rank,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"model_architecture\": model\n",
    "})\n",
    "\n",
    "\n",
    "log_dir = os.path.join(\"./../models/logs/Experiments\")\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 44/44 [01:08<00:00,  1.57s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iter: 43, Train Loss: 1.4443, Val Loss: 1.4467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 44/44 [01:07<00:00,  1.53s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iter: 43, Train Loss: 1.4514, Val Loss: 1.4449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 44/44 [01:07<00:00,  1.52s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iter: 43, Train Loss: 1.4346, Val Loss: 1.4421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 44/44 [01:06<00:00,  1.52s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Iter: 43, Train Loss: 1.4197, Val Loss: 1.4365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 44/44 [01:06<00:00,  1.51s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Iter: 43, Train Loss: 1.4131, Val Loss: 1.4267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 44/44 [01:07<00:00,  1.54s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Iter: 43, Train Loss: 1.3955, Val Loss: 1.4112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 44/44 [01:07<00:00,  1.53s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Iter: 43, Train Loss: 1.3717, Val Loss: 1.3932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 44/44 [01:07<00:00,  1.52s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Iter: 43, Train Loss: 1.3446, Val Loss: 1.3755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 44/44 [01:05<00:00,  1.48s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Iter: 43, Train Loss: 1.3303, Val Loss: 1.3591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 44/44 [01:05<00:00,  1.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Iter: 43, Train Loss: 1.3152, Val Loss: 1.3464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 44/44 [01:05<00:00,  1.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Iter: 43, Train Loss: 1.2841, Val Loss: 1.3365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 44/44 [01:05<00:00,  1.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Iter: 43, Train Loss: 1.2532, Val Loss: 1.3295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 44/44 [01:06<00:00,  1.50s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Iter: 43, Train Loss: 1.2502, Val Loss: 1.3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 44/44 [01:05<00:00,  1.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Iter: 43, Train Loss: 1.2327, Val Loss: 1.3223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 44/44 [01:05<00:00,  1.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Iter: 43, Train Loss: 1.2099, Val Loss: 1.3227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 44/44 [01:05<00:00,  1.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Iter: 43, Train Loss: 1.1908, Val Loss: 1.3249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 44/44 [01:05<00:00,  1.50s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Iter: 43, Train Loss: 1.1708, Val Loss: 1.3268\n",
      "Early Stopping incurred, after 3 iteration(s) of increasing validation loss. \n",
      "Current validation loss: 1.3268404603004456, best validation loss: 1.32227623462677\n",
      "Total training time: 18.82 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 18.82\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>gradient_norms</td><td>▅▃▄▅▂▂▃▂▁▃▇▃▂▂▃▄▄▂▃▄▃▄▄▃▄▅▅▆▅▄▅▇█▇▇▆▅▄▆▆</td></tr><tr><td>gradients/fc_layers.4.bias</td><td>▄▂▄▄▂▄▄▅▂▁▃▄▄▄▂▃▃▃▃▄▃▃▃▃▄▅▄▃▆▃▄▃▆▃▄▄▄█▄▄</td></tr><tr><td>iteration</td><td>▃▂▃▇▁▇▃█▂▄▅▃▆▂▆▁▂▆▇█▅▅▆▃█▄▅▅▇▄▄▇█▄▅▆▁▂▃▇</td></tr><tr><td>loss/val</td><td>███▇▇▆▅▄▃▂▂▁▁▁▁▁▁</td></tr><tr><td>loss_train</td><td>████▇▇▇▇▇▇▇▇▇▇▇▆▆▅▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▂▁</td></tr><tr><td>training_duration</td><td>▁</td></tr><tr><td>weights/fc_layers.4.bias</td><td>█▆▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>16</td></tr><tr><td>gradient_norms</td><td>0.2275</td></tr><tr><td>gradients/fc_layers.4.bias</td><td>-0.00674</td></tr><tr><td>iteration</td><td>43</td></tr><tr><td>loss/val</td><td>1.32684</td></tr><tr><td>loss_train</td><td>1.17079</td></tr><tr><td>training_duration</td><td>1129.2151</td></tr><tr><td>weights/fc_layers.4.bias</td><td>0.01603</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-fog-5</strong> at: <a href='https://wandb.ai/teddybytesorg/New%20Model/runs/uuuemyui' target=\"_blank\">https://wandb.ai/teddybytesorg/New%20Model/runs/uuuemyui</a><br/> View project at: <a href='https://wandb.ai/teddybytesorg/New%20Model' target=\"_blank\">https://wandb.ai/teddybytesorg/New%20Model</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240920_203810-uuuemyui/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "# Training loop with progress bar\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for num_iter, (features, target) in enumerate(tqdm(train_loader,\n",
    "                                                       desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "                                                       unit=\"batch\")):\n",
    "\n",
    "        optimizer.zero_grad() # zero gradients\n",
    "       \n",
    "        # move to gpu\n",
    "        features = to_device(features, device) \n",
    "        target = to_device(target, device)\n",
    "\n",
    "        pred = model(**features) #foward pass\n",
    "        loss = criterion(pred, target) #mse\n",
    "        loss.backward() #backward pass\n",
    "        optimizer.step() #update grad\n",
    "\n",
    "        last_iteration = True if num_iter == (len(train_loader) - 1) else False\n",
    "        \n",
    "        if last_iteration:\n",
    "            val_loss = calulcate_val_loss(model)\n",
    "            \n",
    "        else:\n",
    "            val_loss = None\n",
    "            \n",
    "        log_performance(model=model,\n",
    "                        epoch=epoch, \n",
    "                        num_iter=num_iter, \n",
    "                        train_loss=loss.item(), \n",
    "                        val_loss=val_loss\n",
    "                        )\n",
    "        \n",
    "    # Check for early stopping condition\n",
    "    stop = early_stopper(val_loss, model)\n",
    "    \n",
    "    if stop:\n",
    "        break\n",
    "    \n",
    "train_duration = log_closing_stats(model, start_time=start_time)\n",
    "\n",
    "# Print the total training time\n",
    "print(f\"Total training time: {train_duration / 60:.2f}\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradient_norms</td><td>▄█▂▂▄▃▁▄▃▂▃▂▃▄▁▃▃▃▂▅▃▃▃▂▅▃▄▃▂▃▃▂▄▃▂▃▄▃▃▄</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_train</td><td>▄▅▆▄▄▄▄▆▇▅▅▇▃▃▄▅▅█▃▆▆▅▄▄▅▁▅▄▄▅▅▅▆▅▄▅▆▄▇▅</td></tr><tr><td>loss_val</td><td>▁</td></tr><tr><td>training_duration</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>gradient_norms</td><td>0.28267</td></tr><tr><td>iteration</td><td>43</td></tr><tr><td>loss_train</td><td>1.14684</td></tr><tr><td>loss_val</td><td>1.33425</td></tr><tr><td>training_duration</td><td>54.63309</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">desert-plasma-82</strong> at: <a href='https://wandb.ai/teddybytesorg/base_model/runs/1bbm3dbt' target=\"_blank\">https://wandb.ai/teddybytesorg/base_model/runs/1bbm3dbt</a><br/> View project at: <a href='https://wandb.ai/teddybytesorg/base_model' target=\"_blank\">https://wandb.ai/teddybytesorg/base_model</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240920_175857-1bbm3dbt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Close session\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%:%M%:%S\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-09-19_22:43:06'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate random experiment names\n",
    "def random_experiment_name():\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=8))\n",
    "\n",
    "trying = random_experiment_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WSUnp3qW'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.load('/Users/matthewaudley/Documents/Machine Learning Projects/Business Recommendation System/data/processed/Temp Tensors/input_ids.pt')\n",
    "att = torch.load('/Users/matthewaudley/Documents/Machine Learning Projects/Business Recommendation System/data/processed/Temp Tensors/mask.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.73 GB, other allocations: 656.00 KB, max allowed: 18.13 GB). Tried to allocate 5.24 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m distilbert_model \u001b[38;5;241m=\u001b[39m DistilBertEmb()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m DistilBertEmb()  \u001b[38;5;66;03m# Move to MPS if available\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Assuming `tokens` and `att_masks` are your input tensors\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m embeddings_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilbert_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Save the resulting embeddings tensor\u001b[39;00m\n\u001b[1;32m     60\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(embeddings_tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings_tensor.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m, in \u001b[0;36mcompute_embeddings\u001b[0;34m(token_tensor, att_mask_tensor, distilbert_model, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Move the tensors to the appropriate device (MPS if available)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m token_tensor \u001b[38;5;241m=\u001b[39m token_tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m token_tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m att_mask_tensor \u001b[38;5;241m=\u001b[39m \u001b[43matt_mask_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m att_mask_tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Loop over batches of data\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, token_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), batch_size)):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Get the current batch\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 17.73 GB, other allocations: 656.00 KB, max allowed: 18.13 GB). Tried to allocate 5.24 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DistilBertEmb(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        # Freeze parameters, do not want to fine-tune DistilBERT\n",
    "        for param in self.distilbert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, tokens, att_mask):\n",
    "        # Forward pass\n",
    "        outputs = self.distilbert_model(tokens, attention_mask=att_mask)\n",
    "        # Get embeddings\n",
    "        emb = outputs.last_hidden_state\n",
    "        # Mean Pooling\n",
    "        pooled_tokens_emb = emb.mean(dim=1)  # Across sequence dimension\n",
    "        return pooled_tokens_emb\n",
    "\n",
    "def compute_embeddings(token_tensor, att_mask_tensor, distilbert_model, batch_size=128):  \n",
    "    # Ensure the model is in eval mode\n",
    "    distilbert_model.eval()\n",
    "\n",
    "    # List to store embeddings\n",
    "    all_embeddings = []\n",
    "\n",
    "    # Move the tensors to the appropriate device (MPS if available)\n",
    "    token_tensor = token_tensor.to('mps') if torch.backends.mps.is_available() else token_tensor.to('cpu')\n",
    "    att_mask_tensor = att_mask_tensor.to('mps') if torch.backends.mps.is_available() else att_mask_tensor.to('cpu')\n",
    "\n",
    "    # Loop over batches of data\n",
    "    for i in tqdm(range(0, token_tensor.size(0), batch_size)):\n",
    "        # Get the current batch\n",
    "        batch_tokens = token_tensor[i:i + batch_size]\n",
    "        batch_att_mask = att_mask_tensor[i:i + batch_size]\n",
    "\n",
    "        # Compute embeddings with mixed precision\n",
    "        with torch.no_grad():  # Disable gradients for faster computation\n",
    "            with torch.cuda.amp.autocast():  # Use mixed precision\n",
    "                embeddings = distilbert_model(batch_tokens, batch_att_mask)\n",
    "\n",
    "        # Append the embeddings to the list\n",
    "        all_embeddings.append(embeddings.cpu())  # Move embeddings to CPU for storage\n",
    "\n",
    "    # Concatenate all embeddings into a single tensor\n",
    "    embeddings_tensor = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "    return embeddings_tensor\n",
    "\n",
    "# Usage example\n",
    "# Initialize your DistilBERT embedding model\n",
    "distilbert_model = DistilBertEmb().to('mps') if torch.backends.mps.is_available() else DistilBertEmb()  # Move to MPS if available\n",
    "\n",
    "# Assuming `tokens` and `att_masks` are your input tensors\n",
    "embeddings_tensor = compute_embeddings(tokens, att, distilbert_model)\n",
    "\n",
    "# Save the resulting embeddings tensor\n",
    "torch.save(embeddings_tensor, 'embeddings_tensor.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m wait_time \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m120\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Wait for the random amount of time before scrolling again\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyautogui\n",
    "import random\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    # Generate a random scroll amount between -3 and 3\n",
    "    scroll_amount = random.randint(-3, 3)\n",
    "    \n",
    "    # Scroll the mouse\n",
    "    pyautogui.scroll(scroll_amount)\n",
    "    \n",
    "    # Generate a random wait time between 30 and 120 seconds\n",
    "    wait_time = random.randint(30, 120)\n",
    "    \n",
    "    # Wait for the random amount of time before scrolling again\n",
    "    time.sleep(wait_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogui\n",
      "  Downloading PyAutoGUI-0.9.54.tar.gz (61 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pymsgbox (from pyautogui)\n",
      "  Downloading PyMsgBox-1.0.9.tar.gz (18 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytweening>=1.0.4 (from pyautogui)\n",
      "  Downloading pytweening-1.2.0.tar.gz (171 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyscreeze>=0.1.21 (from pyautogui)\n",
      "  Downloading pyscreeze-1.0.1.tar.gz (27 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pygetwindow>=0.0.5 (from pyautogui)\n",
      "  Downloading PyGetWindow-0.0.9.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mouseinfo (from pyautogui)\n",
      "  Downloading MouseInfo-0.1.3.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyobjc-core (from pyautogui)\n",
      "  Downloading pyobjc_core-10.3.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-quartz (from pyautogui)\n",
      "  Downloading pyobjc_framework_Quartz-10.3.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting pyrect (from pygetwindow>=0.0.5->pyautogui)\n",
      "  Downloading PyRect-0.2.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rubicon-objc (from mouseinfo->pyautogui)\n",
      "  Downloading rubicon_objc-0.4.9-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting pyperclip (from mouseinfo->pyautogui)\n",
      "  Downloading pyperclip-1.9.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyobjc-framework-Cocoa>=10.3.1 (from pyobjc-framework-quartz->pyautogui)\n",
      "  Downloading pyobjc_framework_Cocoa-10.3.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (2.3 kB)\n",
      "Downloading pyobjc_core-10.3.1-cp312-cp312-macosx_10_9_universal2.whl (825 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m826.0/826.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyobjc_framework_Quartz-10.3.1-cp312-cp312-macosx_10_9_universal2.whl (227 kB)\n",
      "Downloading pyobjc_framework_Cocoa-10.3.1-cp312-cp312-macosx_10_9_universal2.whl (396 kB)\n",
      "Downloading rubicon_objc-0.4.9-py3-none-any.whl (63 kB)\n",
      "Building wheels for collected packages: pyautogui, pygetwindow, pyscreeze, pytweening, mouseinfo, pymsgbox, pyperclip, pyrect\n",
      "  Building wheel for pyautogui (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyautogui: filename=PyAutoGUI-0.9.54-py3-none-any.whl size=37577 sha256=0fb72e4435bbe1302634eee477458a16574c2376bce909de3dda440d9089822b\n",
      "  Stored in directory: /Users/matthewaudley/Library/Caches/pip/wheels/d9/d6/47/04075995b093ecc87c212c9a3dbd34e59456c6fe504d65c3e4\n",
      "  Building wheel for pygetwindow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pygetwindow: filename=PyGetWindow-0.0.9-py3-none-any.whl size=11064 sha256=1adc180b333ab7a5fd9d5c845bfe36e673d9496a8fd8fba48a06d094879ae5e4\n",
      "  Stored in directory: /Users/matthewaudley/Library/Caches/pip/wheels/b3/39/81/34dd7a2eca5f885f1f6e2796761970daf66a2d98ac1904f5f4\n",
      "  Building wheel for pyscreeze (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyscreeze: filename=PyScreeze-1.0.1-py3-none-any.whl size=14364 sha256=03944d612f134004389855f31d08a0e2ac9981bdba7060ee368bc7a726c7587b\n",
      "  Stored in directory: /Users/matthewaudley/Library/Caches/pip/wheels/cd/3a/c2/7f2839239a069aa3c9564f6777cbb29d733720ef673f104f0d\n",
      "  Building wheel for pytweening (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytweening: filename=pytweening-1.2.0-py3-none-any.whl size=8011 sha256=5c5be452328cec838981412c18d165c68584fb307b80e5296c0fd73dd5007196\n",
      "  Stored in directory: /Users/matthewaudley/Library/Caches/pip/wheels/23/d5/13/4e9bdadbfe3c78e47c675e7410c0eed2fbb63c5ea6cf1b40e7\n",
      "  Building wheel for mouseinfo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mouseinfo: filename=MouseInfo-0.1.3-py3-none-any.whl size=10889 sha256=55f0098dcc7923b8890232180c11a94af9fb74dc29d94bf4e8eb7ea8a5a7027a\n",
      "  Stored in directory: /Users/matthewaudley/Library/Caches/pip/wheels/b1/9b/f3/08650eb7f00af32f07789f3c6a101e0d7fc762b9891ae843bb\n",
      "  Building wheel for pymsgbox (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pymsgbox: filename=PyMsgBox-1.0.9-py3-none-any.whl size=7406 sha256=341ccd1332d7bdb10c9ea42bc01b1160a2453ecf9fe19618e4b7657c6466c85d\n",
      "  Stored in directory: /Users/matthewaudley/Library/Caches/pip/wheels/55/e7/aa/239163543708d1e15c3d9a1b89dbfe3954b0929a6df2951b83\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.9.0-py3-none-any.whl size=11004 sha256=de797bee79827687a2e6b24b5dea16cca93c4735b1265ae4402d023839bb9788\n",
      "  Stored in directory: /Users/matthewaudley/Library/Caches/pip/wheels/e0/e8/fc/8ab8aa326e33bc066ccd5f3ca9646eab4299881af933f94f09\n",
      "  Building wheel for pyrect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyrect: filename=PyRect-0.2.0-py2.py3-none-any.whl size=11180 sha256=f7bc935da02766381978b6ba74d6927a9075008bcff9aa470d9e4b0f36de9996\n",
      "  Stored in directory: /Users/matthewaudley/Library/Caches/pip/wheels/0b/1e/d7/0c74bd8f60b39c14d84e307398786002aa7ddc905927cc03c5\n",
      "Successfully built pyautogui pygetwindow pyscreeze pytweening mouseinfo pymsgbox pyperclip pyrect\n",
      "Installing collected packages: pytweening, pyscreeze, pyrect, pyperclip, pymsgbox, rubicon-objc, pyobjc-core, pygetwindow, pyobjc-framework-Cocoa, mouseinfo, pyobjc-framework-quartz, pyautogui\n",
      "Successfully installed mouseinfo-0.1.3 pyautogui-0.9.54 pygetwindow-0.0.9 pymsgbox-1.0.9 pyobjc-core-10.3.1 pyobjc-framework-Cocoa-10.3.1 pyobjc-framework-quartz-10.3.1 pyperclip-1.9.0 pyrect-0.2.0 pyscreeze-1.0.1 pytweening-1.2.0 rubicon-objc-0.4.9\n"
     ]
    }
   ],
   "source": [
    "!pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
