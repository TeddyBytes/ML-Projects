{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegressionMe(): #MULTIVARIATE !!\n",
    "    def __init__(self, num_features):\n",
    "        self.weights = np.random.rand(num_features)\n",
    "        self.bias = 1\n",
    "        self.backprop_params = {}\n",
    "        \n",
    "        \n",
    "    def MSE_loss(self, pred, true):\n",
    "        num_observations = len(pred)\n",
    "        # Calculate MSE\n",
    "        error = pred - true\n",
    "        error_sq = error ** 2\n",
    "        sse = np.sum(error_sq)\n",
    "        mse = (1 / num_observations) * sse\n",
    "        \n",
    "        # store params necessary for backprop\n",
    "        self.backprop_params = {'error': error}\n",
    "        \n",
    "        return mse, num_observations\n",
    "    \n",
    "    def manual_backprop(self, X):\n",
    "        \n",
    "        m = X.shape[0]  # Number of samples\n",
    "\n",
    "        # local gradients\n",
    "        dLoss_dMse = 1 # redundant just to show loss is mse \n",
    "        dMse_dSse = 1 / m\n",
    "        dSse_dError_sq = np.ones(m)\n",
    "        dErrorSq_derror = 2 * self.backprop_params['error']\n",
    "        dError_dPred = 1\n",
    "        \n",
    "        # chain rule\n",
    "        dloss_dpred = dLoss_dMse * dMse_dSse * dSse_dError_sq * dErrorSq_derror * dError_dPred\n",
    "        \n",
    "        \n",
    "        dpred_dweights = X\n",
    "        dpred_dbias = 1\n",
    "        \n",
    "        # Final gradient calculation\n",
    "        dloss_dweights = dpred_dweights.T @ dloss_dpred \n",
    "        dloss_dbias = np.sum(dloss_dpred*dpred_dbias)\n",
    "        \n",
    "        return dloss_dweights, dloss_dbias\n",
    "    \n",
    "    \n",
    "    # Gradient of loss with respect to W and b\n",
    "    def compute_gradients(self, X, y_true, y_pred):\n",
    "        m = X.shape[0]  # Number of samples\n",
    "\n",
    "        # gradient of the loss w.r.t. W\n",
    "        dW = -(2/m) * X.T @ (y_true - y_pred)\n",
    "        \n",
    "        # gradient of the loss w.r.t. b\n",
    "        db = -(2/m) * np.sum(y_true - y_pred)\n",
    "\n",
    "        return dW, db\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # store model output, part of foward pass. \n",
    "        num_observations = len(X) # AKA batch size\n",
    "        \n",
    "        # forward pass, calculate prediction result\n",
    "        pred = X @ self.weights + self.bias #weighted sum of inputs, technically affine transformation\n",
    "                                            #same architecture for a single linear layer in a nn\n",
    "        print(pred)\n",
    "        print(y)\n",
    "        \n",
    "        # # calculate loss\n",
    "        loss, num_observations = self.MSE_loss(pred, y) # = np.mean((y - y_pred) ** 2)\n",
    "        true_loss = MSE(pred, y)\n",
    "        # print(loss, true_loss)\n",
    "        \n",
    "        #  perform backward pass\n",
    "        dloss_dweights, dloss_dbias = self.manual_backprop(X)\n",
    "        dloss2, dweights2 = self.compute_gradients(X, y, pred)\n",
    "        \n",
    "        \n",
    "        # print(dloss_dweights, dloss_dbias)\n",
    "        # print(dloss2, dweights2)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
