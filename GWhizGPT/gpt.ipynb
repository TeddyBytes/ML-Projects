{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import List\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Files\n",
    "file_1 = open(f\"./input.txt\", 'r')\n",
    "file_2 = open(f\"./more.txt\", 'r')\n",
    "\n",
    "# Store corpus\n",
    "corpus = file_1.read()\n",
    "corpus += file_2.read()\n",
    "\n",
    "# Store character_set\n",
    "character_set = sorted(list(set(char for char in corpus)))\n",
    "\n",
    "# Store index to char mapping\n",
    "char_to_int = dict()\n",
    "int_to_char = dict()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate mapping\n",
    "for i, char in enumerate(character_set):\n",
    "    # print(char, 2)\n",
    "    char_to_int[char] = i\n",
    "    int_to_char[i] = char\n",
    "\n",
    "\n",
    "# print(len(int_to_char))\n",
    "\n",
    "encoder = lambda text: torch.tensor([char_to_int[char] for char in text], dtype = torch.long)\n",
    "decoder = lambda encoding: ''.join([int_to_char[digit.item()] for digit in encoding])\n",
    "\n",
    "data = encoder(corpus)\n",
    "\n",
    "# Create train/test slip\n",
    "split_idx = int(.9*len(data))\n",
    "train_set = data[:split_idx]\n",
    "test_set = data[split_idx:]\n",
    "\n",
    "block_size = 8\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1012855])\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, window_size, split='train'):\n",
    "    # assign dataset\n",
    "    data = train_set if split == 'train' else train_set\n",
    "    # calculate max index \n",
    "    max_idx = len(train_set)  - block_size \n",
    "    # pick n = batch_size random indices\n",
    "    idx = torch.randint(low=0, high=max_idx, size=(batch_size,)) # does not include max, so high =  max - 1 implicitly\n",
    "    \n",
    "    # Collect training and target sequences\n",
    "    train_seq = torch.stack([torch.tensor(data[i:i + window_size]) for i in idx])\n",
    "    target_seq = torch.stack([torch.tensor(data[i + 1:i + window_size + 1]) for i in idx])\n",
    "\n",
    "    return train_seq, target_seq.view(-1) #THIS WAS FOR BIGRAM MODEL, THE VIEW FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = get_batch()\n",
    "# # print(x, y)\n",
    "# assert x.dtype == y.dtype == torch.long , \"dtypes isnt long\"\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(int_to_char)\n",
    "batch_size = 32\n",
    "# block_size = block_size #this is just to show its referenced earlier\n",
    "d_model = 512 #emb and concat, DIM of Input\n",
    "d_k = 16 #ind head\n",
    "d_v = d_k\n",
    "num_heads = 4\n",
    "num_blocks = 1\n",
    "num_of_iter = 5000\n",
    "dropout = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, key_dimension, value_dimension=None):\n",
    "        super().__init__()\n",
    "        # Store dimension of Q, K, V\n",
    "\n",
    "        self.key_dimension = key_dimension\n",
    "        self.value_dimension = key_dimension if not value_dimension else value_dimension\n",
    "        \n",
    "        # Define layers\n",
    "        self.query = nn.Linear(d_model, self.key_dimension)\n",
    "        self.key = nn.Linear(d_model, self.key_dimension)\n",
    "        self.value = nn.Linear(d_model, self.value_dimension)\n",
    "        \n",
    "        \n",
    "        # Register buffer for the lower triangular mask\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Data will include embedding dimension\n",
    "        B, T, C = x.shape  #c = d_model\n",
    "\n",
    "        # Linear Transformations\n",
    "        query = self.query(x) #(B, T, C)\n",
    "        key = self.key(x) #(B, T, C)\n",
    "        value = self.value(x) #(B, T, C)\n",
    "        # print(query.shape, key.shape, value.shape)\n",
    "        # Scaled dot-product attention\n",
    "        # Reflects the similiarity between the query and the keys\n",
    "        dp_att = query @ torch.transpose(key, -2, -1) #Dot Product of query and value\n",
    "        dp_att *= self.key_dimension ** -.5 #Scaled\n",
    "        \n",
    "        # Masking\n",
    "        # Ensures in decoder architecture, that tokens attend to only past values\n",
    "        mask = self.tril[:T, :T]  # Ensure the mask matches the size of the scores\n",
    "        dp_att_masked = torch.masked_fill(dp_att, mask == 0, float('-inf'))\n",
    "        # # print('hey')\n",
    "        # Softmax normalization\n",
    "        # Normalizes the range of values, represted as probabilitiues \n",
    "        norm = F.softmax(dp_att_masked, -1)\n",
    "        # norm = F.softmax(dp_att, -1)\n",
    "        # print(norm)\n",
    "        out = norm @ value #T x d_v\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, key_dimension, value_dimension=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(key_dimension, value_dimension) for _ in range(num_heads)])\n",
    "        self.fc1 = nn.Linear(num_heads*key_dimension, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # len is num_heads\n",
    "        # head.weights.shape = d_model x d_k, \n",
    "        # head.shape T, d_k\n",
    "        # print(1)\n",
    "        result = [head(x) for head in self.heads]\n",
    "        # print(2)\n",
    "        concat_res = torch.concat(result, dim=-1) # shape = T x d_model=key_dim * num_heads, \n",
    "        # print(concat_res.shape)\n",
    "        # print(3)\n",
    "        output = self.fc1(concat_res)\n",
    "        output = self.dropout(output)\n",
    "        # print(4)\n",
    "        # print(output.shape)\n",
    "        return output #T x d_model\n",
    "        \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads, key_dimension, value_dimension=None):\n",
    "        super().__init__()\n",
    "        # head_size = d_model // num_heads\n",
    "        self.attention = MultiHeadAttention(num_heads, key_dimension, value_dimension)\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, 4*d_model), nn.ReLU(), nn.Linear(4*d_model, d_model), nn.Dropout(dropout))\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model) \n",
    "        self.position_emb = nn.Embedding(block_size, d_model) \n",
    "        # self.head = Head(d_k) \n",
    "        # self.MultiAtt = MultiHeadAttention(num_heads, d_k, d_v) block outputs dim = d_model\n",
    "        self.block = nn.Sequential(Block(num_heads, d_k, d_v), \n",
    "                                   Block(num_heads, d_k, d_v),\n",
    "                                   Block(num_heads, d_k, d_v), \n",
    "                                   nn.LayerNorm(d_model))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, d_model) # align these with architechture\n",
    "        self.fc2 = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "        \n",
    "    def forward(self, x:torch.tensor): # x = (B, T)\n",
    "        \n",
    "        B, T = x.shape\n",
    "        x_emb = self.token_emb(x) #(B, T, C) C = d_model\n",
    "        x_pos = self.position_emb(torch.arange(T))# (T, C) C = d_model\n",
    "        x = x_emb + x_pos #x_pos is broadcasted(1 * B, T, C)\n",
    "        \n",
    "        # # single attention head\n",
    "        # att = self.head(x)   \n",
    "        \n",
    "        # Multi attention head\n",
    "        # att = self.MultiAtt(x)\n",
    "        # print(att.shape) # txd_model\n",
    "        # print('one')\n",
    "        \n",
    "        # Blocks of multiAttHeads\n",
    "        att = self.block(x)\n",
    "        # print(att.shape)\n",
    "        fc1 = self.fc1(att) #   Right aline trailing dimensions\n",
    "        # print('two')\n",
    "        fc2 = self.fc2(fc1)\n",
    "        \n",
    "        return fc2\n",
    "    \n",
    "\n",
    "def generate_next_token(model, curr_seq_enc: torch.tensor, prediction_length: int=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(prediction_length):\n",
    "            idx = curr_seq_enc[:,-1].view(-1, 1).to(torch.long)\n",
    "            logits = LM(idx) \n",
    "            softmax = F.softmax(logits)\n",
    "            # # print(softmax.shape)\n",
    "            sample_token = torch.multinomial(softmax, num_samples=1)\n",
    "            # # print(sample_token, curr_seq_enc)\n",
    "            curr_seq_enc = torch.concat((curr_seq_enc, sample_token), dim=1)\n",
    "            # print(curr_seq_enc)\n",
    "    return curr_seq_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LM()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Store Loss across iterations\n",
    "losses = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 2.0235\n",
      "Iteration 1000, Loss: 1.8927\n",
      "Iteration 2000, Loss: 2.0142\n",
      "Iteration 3000, Loss: 1.8402\n",
      "Iteration 4000, Loss: 1.8628\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(num_of_iter):\n",
    "    # Get batch\n",
    "    inputs, targets = get_batch(batch_size, block_size)\n",
    "    # print(inputs.shape, targets.shape)\n",
    "    # print(inputs.shape, targets.shape)\n",
    "    # # foward pass\n",
    "    logits = model(inputs) #(B*T, C)\n",
    "    # print(logits.shape) \n",
    "    # print(logits)\n",
    "    # logits = logits.squeeze() #THIS IS FOR BIGRAM MODEL \n",
    "    # # print(logits.shape)\n",
    "    # # print(logits.shape, targets.shape)\n",
    "    loss = criterion(logits.view(-1, vocab_size) , targets)\n",
    "    # # print(loss.item())\n",
    "    # # Backward pass\n",
    "    optimizer.zero_grad()  \n",
    "    loss.backward()  # Compute gradients\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss every 100 iterations\n",
    "    if iteration % 1000 == 0:\n",
    "        print(f\"Iteration {iteration}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.341407299041748"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Generate new array that is average of previous and current tokens\n",
    "# for col in range(testing.shape[0]):\n",
    "#     run_sum = 0\n",
    "#     for row in range(testing.shape[1]):\n",
    "#         run_sum = (testing[col][row] + (run_sum))\n",
    "#         result[col][row] = run_sum / (row + 1)\n",
    "        \n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      " [ 8.41470985e-01  9.50415280e-01  9.98334166e-02  9.99500042e-01\n",
      "   9.99983333e-03  9.99995000e-01  9.99999833e-04  9.99999950e-01]\n",
      " [ 9.09297427e-01  8.06578410e-01  1.98669331e-01  9.98000667e-01\n",
      "   1.99986667e-02  9.99980000e-01  1.99999867e-03  9.99999800e-01]\n",
      " [ 1.41120008e-01  5.82753611e-01  2.95520207e-01  9.95503374e-01\n",
      "   2.99955002e-02  9.99955000e-01  2.99999550e-03  9.99999550e-01]\n",
      " [-7.56802495e-01  3.01137463e-01  3.89418342e-01  9.92010661e-01\n",
      "   3.99893342e-02  9.99920001e-01  3.99998933e-03  9.99999200e-01]\n",
      " [-9.58924275e-01 -1.03423189e-02  4.79425539e-01  9.87526020e-01\n",
      "   4.99791693e-02  9.99875003e-01  4.99997917e-03  9.99998750e-01]\n",
      " [-2.79415498e-01 -3.20796458e-01  5.64642473e-01  9.82053935e-01\n",
      "   5.99640065e-02  9.99820005e-01  5.99996400e-03  9.99998200e-01]\n",
      " [ 6.56986599e-01 -5.99437393e-01  6.44217687e-01  9.75599878e-01\n",
      "   6.99428473e-02  9.99755010e-01  6.99994283e-03  9.99997550e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_positional_encoding(sequence_length, embedding_dim):\n",
    "    positional_encoding = np.zeros((sequence_length, embedding_dim))\n",
    "    for pos in range(sequence_length):\n",
    "        for i in range(0, embedding_dim, 2):\n",
    "            positional_encoding[pos, i] = np.sin(pos / (10000 ** (i / embedding_dim)))\n",
    "            positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((i + 1) / embedding_dim)))\n",
    "    return positional_encoding\n",
    "\n",
    "# Example usage for sequence length 8 and embedding dimension 8\n",
    "sequence_length = 8\n",
    "embedding_dim = 8\n",
    "positional_encoding = get_positional_encoding(sequence_length, embedding_dim)\n",
    "\n",
    "print(positional_encoding)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
