## Bigram
- **Description:** A manual implementation of a bigram model designed for character-level text generation. The model utilizes a heatmap to visualize the frequency of character pairs (bigrams) and includes sampling methods for generating new words based on learned probabilities.

### Key Components
1. **Data Visualization:**
   - The bigram frequency is represented in a heatmap using Seaborn, where:
     - The x-axis corresponds to the second character in each bigram.
     - The y-axis corresponds to the first character.
     - The heatmap provides a visual representation of the frequency of character pairs, aiding in understanding the relationships between characters.

2. **Character Sampling:**
   - A generator object is created to sample characters based on the normalized bigram probabilities:
     - The data tensor is converted to a numpy array and normalized to ensure valid probability distributions.
     - Words are generated by starting with a special token (`<S>`) and sampling characters until a terminating token (`<E>`) is reached. The generated words are stored for further analysis.

3. **Data Preparation for Training:**
   - Input and target sequences are constructed from training data by:
     - Adding start (`<S>`) and end (`<E>`) tokens.
     - Creating a list of bigrams and converting them to one-hot encoded vectors for both inputs and targets.

4. **Model Training:**
   - The model employs a weight matrix initialized randomly, which is trained through:
     - Computing logits as unnormalized probabilities from the input representations.
     - Normalizing these logits into probability distributions using softmax.
     - Calculating the negative log likelihood as a loss function, which is used to optimize the model during training.

5. **Loss Calculation:**
   - The model compares its calculated loss with cross-entropy loss for performance evaluation. This helps in ensuring the model is learning correctly from the training data.

### Visualizations and Results
- The heatmap generated provides insights into the character relationships, while the word generation component allows for exploring the model's ability to create coherent text based on learned patterns.

### Usage
- To utilize this bigram model, run the scripts provided in this repository, ensuring you have the necessary libraries installed (e.g., PyTorch, NumPy, Seaborn, and Matplotlib).


